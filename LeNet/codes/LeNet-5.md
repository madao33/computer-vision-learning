## å®šä¹‰LeNet-5ç½‘ç»œ

# LeNet-5ç½‘ç»œä»¿çœŸå®ç°

## LeNet-5ç½‘ç»œæ¶æ„ä»‹ç»

`LeNet-5`ç½‘ç»œæ¶æ„å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œä¸åŒ…æ‹¬è¾“å…¥å±‚çš„è¯ï¼Œ`LeNet-5`ä¸€å…±æœ‰7å±‚ï¼Œæ‰€æœ‰çš„å±‚éƒ½æœ‰å¯ä»¥è®­ç»ƒçš„å‚æ•°(æƒé‡).
![](../imgs/lenet5.png)
è¾“å…¥çš„å›¾åƒæ˜¯  $32\times32$ å°ºå¯¸çš„ç‚¹é˜µå›¾ï¼Œè¿™æ¯”ç”¨äºè®­ç»ƒçš„æ•°æ®åº“ä¸­çš„æœ€å¤§çš„å­—ç¬¦è¿˜è¦å¤§(æ•°æ®åº“ä¸­çš„å¤§å¤šæ•°æ•°æ®å°ºå¯¸åœ¨ $20\times20$â€‹â€‹â€‹â€‹)ã€‚è¿™æ ·åšçš„åŸå› æ˜¯æœŸæœ›è¯†åˆ«åˆ°æ½œåœ¨çš„åŒºåˆ«ç‰¹å¾ï¼Œä¾‹å¦‚ç¬”åˆ’ç»ˆç‚¹æˆ–è½¬è§’å¯ä»¥å‡ºç°åœ¨æœ€é«˜æ°´å¹³ç‰¹å¾æ£€æµ‹å™¨çš„æ„Ÿå—é‡çš„ä¸­å¿ƒã€‚åœ¨ $32\times32$â€‹çš„è¾“å…¥æ•°æ®ä¸­ï¼ŒLeNet-5ç½‘ç»œçš„æœ€åä¸€å±‚å·ç§¯å±‚çš„æ„Ÿå—é‡ä¸­å¿ƒå½¢æˆ $20\times20$â€‹çš„åŒºåŸŸã€‚

è¾“å…¥çš„æ•°æ®ç»è¿‡å½’ä¸€åŒ–ï¼Œç™½è‰²çš„ç‚¹åœ¨-0.1ï¼Œé»‘è‰²çš„ç‚¹åœ¨1.175ï¼Œè¿™æ ·è®©è¾“å…¥çš„å¹³å‡å€¼åœ¨0å·¦å³ï¼Œæ–¹å·®åœ¨1å·¦å³ï¼Œå¯ä»¥åŠ é€Ÿå­¦ä¹ ã€‚


è¿™ä¸ªç½‘ç»œæ¶æ„è™½ç„¶æ¯”è¾ƒå°ï¼Œä½†æ˜¯ä¹ŸåŒ…å«äº†æ·±åº¦å­¦ä¹ çš„ä¸»è¦çš„åŸºæœ¬æ¨¡å—ï¼š

### input layer

æ•°æ®è¾“å…¥å±‚ï¼Œå°†è¾“å…¥å›¾åƒå°ºå¯¸ç»Ÿä¸€å¹¶å½’ä¸€åŒ–ä¸º $32\times32$

### c1å·ç§¯å±‚(convolutional layer)

* è¾“å…¥ï¼š$32 \times 32$

* å·ç§¯æ ¸å¤§å°ï¼š$5\times5, s=1$â€‹

* å·ç§¯ç§ç±»ï¼š6

* ç¥ç»å…ƒæ•°é‡ï¼š$28\times28\times6$

* å¯è®­ç»ƒå‚æ•°ï¼š$(5\times5+1)\times6=156$â€‹ 

  > æ¯ä¸ªæ»¤æ³¢å™¨ $5\times5=25$â€‹ä¸ª`unit`å‚æ•°å’Œä¸€ä¸ª`bias`å‚æ•°ï¼Œä¸€å…±6ä¸ªæ»¤æ³¢å™¨

* è¿æ¥æ•°ï¼š$(5\times5+1)\times6\times28\times28=122304$â€‹

  > å·ç§¯å±‚`C1`å†…çš„æ¯ä¸ªåƒç´ éƒ½ä¸è¾“å…¥å›¾åƒä¸­çš„ $5\times5$â€‹ä¸ªåƒç´ å’Œ1ä¸ªbiasæœ‰è¿æ¥ï¼Œæ‰€ä»¥æ€»å…±æœ‰ $156\times28\times28=12304$â€‹ ä¸ªè¿æ¥ç‚¹

> å¯¹è¾“å…¥å›¾åƒè¿›è¡Œç¬¬ä¸€æ¬¡å·ç§¯è¿ç®—ï¼ˆä½¿ç”¨ 6 ä¸ªå¤§å°ä¸º $5\times5$â€‹â€‹â€‹â€‹ çš„å·ç§¯æ ¸ï¼‰ï¼Œå¾—åˆ°6ä¸ª`C1`ç‰¹å¾å›¾ï¼ˆ6ä¸ªå¤§å°ä¸º$28\times28$â€‹â€‹çš„ `feature maps`, 32-5+1=28ï¼‰ã€‚æˆ‘ä»¬å†æ¥çœ‹çœ‹éœ€è¦å¤šå°‘ä¸ªå‚æ•°ï¼Œå·ç§¯æ ¸çš„å¤§å°ä¸º$5\times5$â€‹â€‹ï¼Œæ€»å…±å°±æœ‰$6\times(5\times5+1)=156$â€‹â€‹â€‹ä¸ªå‚æ•°ï¼Œå…¶ä¸­+1æ˜¯è¡¨ç¤ºä¸€ä¸ªæ ¸æœ‰ä¸€ä¸ª`bias`ã€‚å¯¹äºå·ç§¯å±‚`C1`ï¼Œ`C1`å†…çš„æ¯ä¸ªåƒç´ éƒ½ä¸è¾“å…¥å›¾åƒä¸­çš„$5\times5$â€‹â€‹ä¸ªåƒç´ å’Œ1ä¸ª`bias`æœ‰è¿æ¥ï¼Œæ‰€ä»¥æ€»å…±æœ‰$156\times28\times28=122304$â€‹â€‹â€‹ä¸ªè¿æ¥ï¼ˆconnectionï¼‰ã€‚æœ‰122304ä¸ªè¿æ¥ï¼Œä½†æ˜¯æˆ‘ä»¬åªéœ€è¦å­¦ä¹ 156ä¸ªå‚æ•°ï¼Œä¸»è¦æ˜¯é€šè¿‡æƒå€¼å…±äº«å®ç°çš„ã€‚

### S2æ± åŒ–å±‚(sub-sampS2ling layer)

* è¾“å…¥ï¼š$28\times28$
* é‡‡æ ·åŒºåŸŸï¼š$2\times2$
* é‡‡æ ·æ–¹å¼ï¼š4ä¸ªè¾“å…¥ç›¸åŠ ï¼Œä¹˜ä»¥ä¸€ä¸ªå¯è®­ç»ƒå‚æ•°ï¼Œå†åŠ ä¸Šä¸€ä¸ªå¯è®­ç»ƒåç½®ã€‚ç»“æœé€šè¿‡`sigmoid`å‡½æ•°
* é‡‡æ ·ç§ç±»ï¼š6
* è¾“å‡º`featureMap`å¤§å°ï¼š$14\times14(28/2)$â€‹
* ç¥ç»å…ƒæ•°é‡ï¼š$14\times14\times6$â€‹
* å¯è®­ç»ƒå‚æ•°ï¼š$2\times6$â€‹(å’Œçš„æƒ+åç½®)
* è¿æ¥æ•°ï¼š$(2\times2+1)\times6\times14\times14$â€‹
* `S2`ä¸­æ¯ä¸ªç‰¹å¾å›¾çš„å¤§å°æ˜¯`C1`ä¸­ç‰¹å¾å›¾å¤§å°çš„1/4ã€‚

> ç¬¬ä¸€æ¬¡å·ç§¯ä¹‹åç´§æ¥ç€å°±æ˜¯æ± åŒ–è¿ç®—ï¼Œä½¿ç”¨ $2\times 2$â€‹â€‹â€‹æ ¸è¿›è¡Œæ± åŒ–ï¼Œäºæ˜¯å¾—åˆ°äº†`S2`ï¼Œ6ä¸ª$14\times14$â€‹â€‹â€‹â€‹çš„ç‰¹å¾å›¾ï¼ˆ28/2=14ï¼‰ã€‚`S2`è¿™ä¸ªpoolingå±‚æ˜¯å¯¹C1ä¸­çš„ $2\times2$â€‹â€‹â€‹â€‹ åŒºåŸŸå†…çš„åƒç´ æ±‚å’Œä¹˜ä»¥ä¸€ä¸ªæƒå€¼ç³»æ•°å†åŠ ä¸Šä¸€ä¸ªåç½®ï¼Œç„¶åå°†è¿™ä¸ªç»“æœå†åšä¸€æ¬¡æ˜ å°„ã€‚äºæ˜¯æ¯ä¸ªæ± åŒ–æ ¸æœ‰ä¸¤ä¸ªè®­ç»ƒå‚æ•°ï¼Œæ‰€ä»¥å…±æœ‰2x6=12ä¸ªè®­ç»ƒå‚æ•°ï¼Œä½†æ˜¯æœ‰5x14x14x6=5880ä¸ªè¿æ¥ã€‚

### C3å·ç§¯å±‚(convolutional layer)

* `S2`ä¸­æ‰€æœ‰6ä¸ªæˆ–è€…å‡ ä¸ªç‰¹å¾`map`ç»„åˆ
* å·ç§¯æ ¸å¤§å°ï¼š$5\times5$
* å·ç§¯æ ¸ç§ç±»ï¼š16
* è¾“å‡º`featureMap`å¤§å°ï¼š$10\times10(14-5+1)=10$

![](../imgs/table1.png)

`C3`ä¸­çš„æ¯ä¸ªç‰¹å¾`map`æ˜¯è¿æ¥åˆ°`S2`ä¸­çš„æ‰€æœ‰6ä¸ªæˆ–è€…å‡ ä¸ªç‰¹å¾`map`çš„ï¼Œè¡¨ç¤ºæœ¬å±‚çš„ç‰¹å¾`map`æ˜¯ä¸Šä¸€å±‚æå–åˆ°çš„ç‰¹å¾`map`çš„ä¸åŒç»„åˆã€‚å­˜åœ¨çš„ä¸€ä¸ªæ–¹å¼æ˜¯ï¼š`C3`çš„å‰6ä¸ªç‰¹å¾å›¾ä»¥`S2`ä¸­3ä¸ªç›¸é‚»çš„ç‰¹å¾å›¾å­é›†ä¸ºè¾“å…¥ã€‚æ¥ä¸‹æ¥6ä¸ªç‰¹å¾å›¾ä»¥`S2`ä¸­4ä¸ªç›¸é‚»ç‰¹å¾å›¾å­é›†ä¸ºè¾“å…¥ã€‚ç„¶åçš„3ä¸ªä»¥ä¸ç›¸é‚»çš„4ä¸ªç‰¹å¾å›¾å­é›†ä¸ºè¾“å…¥ã€‚æœ€åä¸€ä¸ªå°†`S2`ä¸­æ‰€æœ‰ç‰¹å¾å›¾ä¸ºè¾“å…¥ã€‚åˆ™ï¼šå¯è®­ç»ƒå‚æ•°ï¼š
$$
6\times(3\times5\times5+1)+6\times(4\times5\times5+1)+3\times(4\times5\times5+1)+\times(6\times5\times5+1)=1516
$$
è¿™ç§éå¯¹ç§°çš„è¿æ¥çš„ä½œç”¨æ˜¯ï¼š

* éå®Œå…¨è¿æ¥çš„æ–¹æ¡ˆå¯ä»¥ä½¿è¿æ¥æ•°ä¿æŒåœ¨åˆç†çš„èŒƒå›´å†…
* ä¸åŒçš„ç‰¹å¾å›¾å› ä¸ºæœ‰ä¸åŒçš„ç‰¹å¾å¯ä»¥æå–å¤šç§ç»„åˆ

### S4æ± åŒ–å±‚(sub-sampling layer)

* è¾“å…¥ï¼š$10\times10$
* é‡‡æ ·åŒºåŸŸï¼š$2\times2$
* é‡‡æ ·æ–¹å¼ï¼š4ä¸ªè¾“å…¥ç›¸åŠ ï¼Œä¹˜ä»¥ä¸€ä¸ªå¯è®­ç»ƒå‚æ•°ï¼Œå†åŠ ä¸Šä¸€ä¸ªå¯è®­ç»ƒåç½®ã€‚ç»“æœé€šè¿‡`sigmoid`
* é‡‡æ ·ç§ç±»ï¼š16
* è¾“å‡º`featureMap`å¤§å°ï¼š$5\times5$
* ç¥ç»å…ƒæ•°é‡ï¼š$5\times5\times16=400$
* å¯è®­ç»ƒå‚æ•°ï¼š$2\times16=32$
* è¿æ¥æ•°ï¼š$16\times(2\times2+1)\times5\times5=2000$â€‹
* `S4`ä¸­æ¯ä¸ªç‰¹å¾å›¾çš„å¤§å°æ˜¯`C3`ä¸­ç‰¹å¾å›¾å¤§å°çš„1/4

> `S4`æ˜¯`pooling`å±‚ï¼Œçª—å£å¤§å°ä»ç„¶æ˜¯$2\times2$â€‹â€‹â€‹ï¼Œå…±è®¡16ä¸ª`feature map`ï¼Œ`C3`å±‚çš„16ä¸ª$10\times10$â€‹â€‹â€‹çš„å›¾åˆ†åˆ«è¿›è¡Œä»¥$2\times2$â€‹â€‹â€‹ä¸ºå•ä½çš„æ± åŒ–å¾—åˆ°16ä¸ª $5\times5$â€‹â€‹â€‹ çš„ç‰¹å¾å›¾ã€‚è¿™ä¸€å±‚æœ‰2x16å…±32ä¸ªè®­ç»ƒå‚æ•°ï¼Œ$5\times5\times5\times16=2000$â€‹â€‹ ä¸ªè¿æ¥ã€‚è¿æ¥çš„æ–¹å¼ä¸`S2`å±‚ç±»ä¼¼ã€‚

### C5å·ç§¯å±‚(convolutional layer)

* è¾“å…¥ï¼š`S4`å±‚çš„å…¨éƒ¨16ä¸ªå•å…ƒ`featureMap`(ä¸`S4`å…¨ç›¸è¿)
* å·ç§¯æ ¸å¤§å°ï¼š$5\times5$
* å·ç§¯æ ¸ç§ç±»ï¼š120
* è¾“å‡º`featureMap`å¤§å°ï¼š$1\times1(5-5+1)$
* å¯è®­ç»ƒå‚æ•°/è¿æ¥ï¼š$120\times(16\times5\times5+1)=48120$

> `C5`å±‚æ˜¯ä¸€ä¸ªæœ‰120ä¸ª`featureMap`çš„å·ç§¯å±‚ã€‚æ¯ä¸€ä¸ªå•å…ƒå’Œ`S4`çš„æ‰€æœ‰16ä¸ª`featureMap`çš„ $5\times 5$â€‹ é‚»åŸŸå…¨è¿æ¥ã€‚å› ä¸º`S4`çš„`featureMap`æ˜¯ $5 \times 5$ ï¼Œæ‰€ä»¥`C5`çš„`featureMap`æ˜¯ $1\times1$ ã€‚

### F6å…¨è¿æ¥å±‚(fully-ocnnected layer)

* è¾“å…¥ï¼š`C5` 120ç»´å‘é‡
* è®¡ç®—æ–¹å¼ï¼šè®¡ç®—è¾“å…¥å‘é‡å’Œæƒé‡å‘é‡ä¹‹é—´çš„ç‚¹ç§¯ï¼Œå†åŠ ä¸Šä¸€ä¸ªåç½®ï¼Œç»“æœé€šè¿‡`sigmoid`å‡½æ•°è¾“å‡ºã€‚
* å¯è®­ç»ƒå‚æ•°ï¼š$84\times(120+1)=10164$

åŒ…å«84ä¸ªèŠ‚ç‚¹ï¼Œå¯¹åº”äºä¸€ä¸ª $7\times12$ çš„æ¯”ç‰¹å›¾ï¼Œ$-1$ è¡¨ç¤ºç™½è‰²ï¼Œ $1$â€‹ è¡¨ç¤ºé»‘è‰²ï¼Œè¿™æ ·æ¯ä¸ªç¬¦å·çš„æ¯”ç‰¹å›¾çš„é»‘ç™½è‰²å°±å¯¹åº”äºä¸€ä¸ªç¼–ç ã€‚

![](../imgs/fig3.png)

### output layer

æœ€åï¼Œè¾“å‡ºå±‚ç”±æ¬§å‡ é‡Œå¾—å¾„å‘åŸºå‡½æ•°(Euclidean Radial Basis Function)ç»„æˆï¼Œä¹Ÿæ˜¯å…¨è¿æ¥å±‚ã€‚å…±æœ‰10ä¸ªèŠ‚ç‚¹ï¼Œåˆ†åˆ«è¡¨ç¤º0åˆ°9ï¼Œå¯¹äºæ¯ä¸ª`RBF`å•å…ƒ$y_i$çš„è¾“å‡ºï¼š
$$
y_i = \sum_j(x_j-w_{ij})^2
$$

### æ€»ç»“

ç°åœ¨çš„å¤§å¤šæ•°ç¥ç»ç½‘ç»œæ˜¯é€šè¿‡`softmax`å‡½æ•°è¾“å‡ºå¤šåˆ†ç±»ç»“æœï¼Œç›¸æ¯”äºç°ä»£ç‰ˆæœ¬ï¼Œè¿™é‡Œå¾—åˆ°çš„ç¥ç»ç½‘ç»œä¼šå°ä¸€äº›ï¼Œåªæœ‰çº¦6ä¸‡ä¸ªå‚æ•°ï¼Œç°åœ¨çš„ä¸€äº›ç¥ç»ç½‘ç»œç”šè‡³æœ‰ä¸€åƒä¸‡åˆ°ä¸€äº¿ä¸ªå‚æ•°ã€‚

ä»`LeNet-5`ç½‘ç»œä»å·¦å¾€å³çœ‹ï¼Œéšç€ç½‘ç»œè¶Šæ¥è¶Šæ·±ï¼Œå›¾åƒçš„é«˜åº¦å’Œå®½åº¦éƒ½åœ¨ç¼©å°ï¼Œä»æœ€åˆçš„ $32\times32$â€‹ ç¼©å°åˆ° $28\times28$â€‹ï¼Œå†åˆ° $14\times14$â€‹ã€$10\times10$â€‹ï¼Œæœ€ååªç”¨ $5\times5$â€‹ï¼Œä¸æ­¤åŒæ—¶ï¼Œéšç€ç½‘ç»œå±‚æ¬¡çš„åŠ æ·±ï¼Œé€šé“æ•°é‡ä¸€ç›´åœ¨å¢åŠ ï¼Œä»1å¢åŠ åˆ°6ä¸ªï¼Œå†åˆ°16ä¸ªã€‚

LeNet5ç½‘ç»œçš„ç‰¹åˆ«ä¹‹å¤„è¿˜åœ¨äºï¼Œå„ä¸ªç½‘ç»œä¹‹é—´æ˜¯æœ‰å…³è”çš„ï¼Œæ¯”å¦‚è¯´ï¼Œä½ æœ‰ä¸€ä¸ª$nH\times nW\times nC$â€‹â€‹ çš„ç½‘ç»œï¼Œæœ‰$nC$â€‹â€‹ä¸ªé€šé“ï¼Œä½¿ç”¨å°ºå¯¸ä¸º$ğ‘“ Ã— ğ‘“ Ã— ğ‘› ğ¶ $â€‹çš„è¿‡æ»¤å™¨ï¼Œæ¯ä¸ªè¿‡æ»¤å™¨çš„é€šé“æ•°å’Œå®ƒä¸Šä¸€å±‚çš„é€šé“æ•°ç›¸åŒã€‚è¿™æ˜¯ç”±äºåœ¨å½“æ—¶ï¼Œè®¡ç®—æœºçš„è¿è¡Œé€Ÿåº¦éå¸¸æ…¢ï¼Œä¸ºäº†å‡å°‘è®¡ç®—é‡å’Œå‚æ•°ï¼Œç»å…¸çš„ `LeNet-5 `ç½‘ç»œä½¿ç”¨äº†éå¸¸å¤æ‚çš„è®¡ç®—æ–¹å¼ï¼Œæ¯ä¸ªè¿‡æ»¤å™¨éƒ½é‡‡ç”¨å’Œè¾“å…¥æ¨¡å—ä¸€æ ·çš„é€šé“æ•°é‡ã€‚

## LeNet-5ç½‘ç»œå®šä¹‰


```python
from collections import OrderedDict
import torch.nn as nn
class C1(nn.Module):
    def __init__(self):
        super(C1, self).__init__()

        self.c1 = nn.Sequential(OrderedDict([
            ('c1', nn.Conv2d(1, 6, kernel_size=(5, 5))),
            ('relu1', nn.ReLU()),
            ('s1', nn.MaxPool2d(kernel_size=(2, 2), stride=2))
        ]))

    def forward(self, img):
        output = self.c1(img)
        return output


class C2(nn.Module):
    def __init__(self):
        super(C2, self).__init__()

        self.c2 = nn.Sequential(OrderedDict([
            ('c2', nn.Conv2d(6, 16, kernel_size=(5, 5))),
            ('relu2', nn.ReLU()),
            ('s2', nn.MaxPool2d(kernel_size=(2, 2), stride=2))
        ]))

    def forward(self, img):
        output = self.c2(img)
        return output


class C3(nn.Module):
    def __init__(self):
        super(C3, self).__init__()

        self.c3 = nn.Sequential(OrderedDict([
            ('c3', nn.Conv2d(16, 120, kernel_size=(5, 5))),
            ('relu3', nn.ReLU())
        ]))

    def forward(self, img):
        output = self.c3(img)
        return output


class F4(nn.Module):
    def __init__(self):
        super(F4, self).__init__()

        self.f4 = nn.Sequential(OrderedDict([
            ('f4', nn.Linear(120, 84)),
            ('relu4', nn.ReLU())
        ]))

    def forward(self, img):
        output = self.f4(img)
        return output


class F5(nn.Module):
    def __init__(self):
        super(F5, self).__init__()

        self.f5 = nn.Sequential(OrderedDict([
            ('f5', nn.Linear(84, 10)),
            ('sig5', nn.LogSoftmax(dim=-1))
        ]))

    def forward(self, img):
        output = self.f5(img)
        return output


class LeNet5(nn.Module):
    """
    Input - 1x32x32
    Output - 10
    """
    def __init__(self):
        super(LeNet5, self).__init__()

        self.c1 = C1()
        self.c2_1 = C2() 
        self.c2_2 = C2() 
        self.c3 = C3() 
        self.f4 = F4() 
        self.f5 = F5() 

    def forward(self, img):
        output = self.c1(img)

        x = self.c2_1(output)
        output = self.c2_2(output)

        output += x

        output = self.c3(output)
        output = output.view(img.size(0), -1)
        output = self.f4(output)
        output = self.f5(output)
        return output

```

## æ•°æ®å‡†å¤‡

ä½¿ç”¨çš„æ•°æ®æ˜¯MNISTçš„æ‰‹å†™å­—ç¬¦é›†ï¼Œå¯ä»¥ç›´æ¥é€šè¿‡`torchvision.datasets.mnist`è·å–


```python
from torchvision.datasets.mnist import MNIST
from lenet import LeNet5
import torch
import torch.optim as optim
from torchvision.datasets.mnist import MNIST
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
import visdom
import onnx
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
```


```python
viz = visdom.Visdom()
```

    Setting up a new session...



```python
data_train = MNIST('./data/mnist', 
                    download=True,
                    transform=transforms.transforms.Compose([
                        transforms.Resize((32, 32)), 
                        transforms.transforms.ToTensor()
                    ]))
data_test = MNIST('./data/mnist',
                    train=False,
                    download=True,
                    transform=transforms.transforms.Compose([
                        transforms.Resize((32, 32)),
                        transforms.ToTensor()])
                    )
```

    /home/madao/anaconda3/envs/python3.6env/lib/python3.6/site-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:180.)
      return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)



```python
data_train_loader = DataLoader(data_train, batch_size=256, shuffle=True, num_workers=8)
data_test_loader = DataLoader(data_test, batch_size=1024, num_workers=8)
```


```python
net = LeNet5().to(device)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(net.parameters(), lr=2e-3)
cur_batch_win = None
cur_batch_win_opts = {
    'title': 'Epoch Loss Trace',
    'xlabel': 'Batch Number',
    'ylabel': 'Loss',
    'width': 1200,
    'height': 600,
}
```


```python
def train(epoch):
    global cur_batch_win
    global train_loss
    net.train()
    loss_list, batch_list = [], []
    for i, (images, labels) in enumerate(data_train_loader):
        optimizer.zero_grad()
        images = images.to(device)
        labels = labels.to(device)
        output = net(images)

        loss = criterion(output, labels)

        loss_list.append(loss.detach().cpu())
        batch_list.append(i+1)

        if i % 10 == 0:
            print('Train - Epoch %d, Batch: %d, Loss: %f' % (epoch, i, loss.detach().cpu().item()))

        # Update Visualization
        if viz.check_connection():
            cur_batch_win = viz.line(torch.Tensor(loss_list), torch.Tensor(batch_list),
                                     win=cur_batch_win, name='current_batch_loss',
                                     update=(None if cur_batch_win is None else 'replace'),
                                     opts=cur_batch_win_opts)

        loss.backward()
        optimizer.step()
    return loss_list

```


```python
def test():
    net.eval()
    total_correct = 0
    avg_loss = 0.0

    for i, (images, labels) in enumerate(data_test_loader):
        images = images.to(device)
        labels = labels.to(device)
        output = net(images)

        avg_loss += criterion(output, labels).sum()
        pred = output.detach().cpu().max(1)[1]
        total_correct += pred.eq(labels.cpu().view_as(pred)).sum()

    avg_loss /= len(data_test)
    print('Test Avg. Loss: %f, Accuracy: %f' % (avg_loss.detach().cpu().item(), float(total_correct) / len(data_test)))

```


```python
def train_and_test(epoch):
    train_loss = train(epoch)
    test()

    dummy_input = torch.randn(1, 1, 32, 32, requires_grad=True).to(device)
    torch.onnx.export(net, dummy_input, "lenet.onnx")

    onnx_model = onnx.load("lenet.onnx")
    onnx.checker.check_model(onnx_model)
    return train_loss
```


```python
def main():
    train_losss = list()
    for e in range(1, 16):
        train_loss = train_and_test(e)
        for loss in train_loss:
            train_losss.append(loss)
    return train_losss
```


```python
train_loss = main()
```

    /home/madao/anaconda3/envs/python3.6env/lib/python3.6/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)
      return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)


    Train - Epoch 1, Batch: 0, Loss: 2.311965
    Train - Epoch 1, Batch: 10, Loss: 2.142133
    Train - Epoch 1, Batch: 20, Loss: 1.255652
    Train - Epoch 1, Batch: 30, Loss: 0.770596
    Train - Epoch 1, Batch: 40, Loss: 0.534266
    Train - Epoch 1, Batch: 50, Loss: 0.442306
    Train - Epoch 1, Batch: 60, Loss: 0.468153
    Train - Epoch 1, Batch: 70, Loss: 0.356581
    Train - Epoch 1, Batch: 80, Loss: 0.269619
    Train - Epoch 1, Batch: 90, Loss: 0.256702
    Train - Epoch 1, Batch: 100, Loss: 0.273830
    Train - Epoch 1, Batch: 110, Loss: 0.266757
    Train - Epoch 1, Batch: 120, Loss: 0.200562
    Train - Epoch 1, Batch: 130, Loss: 0.222001
    Train - Epoch 1, Batch: 140, Loss: 0.204945
    Train - Epoch 1, Batch: 150, Loss: 0.205453
    Train - Epoch 1, Batch: 160, Loss: 0.179718
    Train - Epoch 1, Batch: 170, Loss: 0.179102
    Train - Epoch 1, Batch: 180, Loss: 0.135494
    Train - Epoch 1, Batch: 190, Loss: 0.136008
    Train - Epoch 1, Batch: 200, Loss: 0.074584
    Train - Epoch 1, Batch: 210, Loss: 0.078482
    Train - Epoch 1, Batch: 220, Loss: 0.134642
    Train - Epoch 1, Batch: 230, Loss: 0.118460
    Test Avg. Loss: 0.000105, Accuracy: 0.966500
    Train - Epoch 2, Batch: 0, Loss: 0.176212
    Train - Epoch 2, Batch: 10, Loss: 0.153552
    Train - Epoch 2, Batch: 20, Loss: 0.066817
    Train - Epoch 2, Batch: 30, Loss: 0.065422
    Train - Epoch 2, Batch: 40, Loss: 0.154351
    Train - Epoch 2, Batch: 50, Loss: 0.097159
    Train - Epoch 2, Batch: 60, Loss: 0.143552
    Train - Epoch 2, Batch: 70, Loss: 0.077929
    Train - Epoch 2, Batch: 80, Loss: 0.062687
    Train - Epoch 2, Batch: 90, Loss: 0.093121
    Train - Epoch 2, Batch: 100, Loss: 0.046552
    Train - Epoch 2, Batch: 110, Loss: 0.167599
    Train - Epoch 2, Batch: 120, Loss: 0.110213
    Train - Epoch 2, Batch: 130, Loss: 0.065434
    Train - Epoch 2, Batch: 140, Loss: 0.136412
    Train - Epoch 2, Batch: 150, Loss: 0.032096
    Train - Epoch 2, Batch: 160, Loss: 0.089830
    Train - Epoch 2, Batch: 170, Loss: 0.120529
    Train - Epoch 2, Batch: 180, Loss: 0.080730
    Train - Epoch 2, Batch: 190, Loss: 0.081082
    Train - Epoch 2, Batch: 200, Loss: 0.061742
    Train - Epoch 2, Batch: 210, Loss: 0.104012
    Train - Epoch 2, Batch: 220, Loss: 0.075414
    Train - Epoch 2, Batch: 230, Loss: 0.054316
    Test Avg. Loss: 0.000067, Accuracy: 0.978800
    Train - Epoch 3, Batch: 0, Loss: 0.066542
    Train - Epoch 3, Batch: 10, Loss: 0.057625
    Train - Epoch 3, Batch: 20, Loss: 0.062523
    Train - Epoch 3, Batch: 30, Loss: 0.052255
    Train - Epoch 3, Batch: 40, Loss: 0.080742
    Train - Epoch 3, Batch: 50, Loss: 0.042895
    Train - Epoch 3, Batch: 60, Loss: 0.083007
    Train - Epoch 3, Batch: 70, Loss: 0.074603
    Train - Epoch 3, Batch: 80, Loss: 0.055068
    Train - Epoch 3, Batch: 90, Loss: 0.030841
    Train - Epoch 3, Batch: 100, Loss: 0.048446
    Train - Epoch 3, Batch: 110, Loss: 0.090024
    Train - Epoch 3, Batch: 120, Loss: 0.079241
    Train - Epoch 3, Batch: 130, Loss: 0.071417
    Train - Epoch 3, Batch: 140, Loss: 0.060021
    Train - Epoch 3, Batch: 150, Loss: 0.058526
    Train - Epoch 3, Batch: 160, Loss: 0.081278
    Train - Epoch 3, Batch: 170, Loss: 0.050955
    Train - Epoch 3, Batch: 180, Loss: 0.085286
    Train - Epoch 3, Batch: 190, Loss: 0.046122
    Train - Epoch 3, Batch: 200, Loss: 0.027791
    Train - Epoch 3, Batch: 210, Loss: 0.094474
    Train - Epoch 3, Batch: 220, Loss: 0.067521
    Train - Epoch 3, Batch: 230, Loss: 0.034245
    Test Avg. Loss: 0.000054, Accuracy: 0.983600
    Train - Epoch 4, Batch: 0, Loss: 0.048238
    Train - Epoch 4, Batch: 10, Loss: 0.047161
    Train - Epoch 4, Batch: 20, Loss: 0.057948
    Train - Epoch 4, Batch: 30, Loss: 0.026137
    Train - Epoch 4, Batch: 40, Loss: 0.055747
    Train - Epoch 4, Batch: 50, Loss: 0.062449
    Train - Epoch 4, Batch: 60, Loss: 0.058243
    Train - Epoch 4, Batch: 70, Loss: 0.041330
    Train - Epoch 4, Batch: 80, Loss: 0.029140
    Train - Epoch 4, Batch: 90, Loss: 0.055626
    Train - Epoch 4, Batch: 100, Loss: 0.048235
    Train - Epoch 4, Batch: 110, Loss: 0.040169
    Train - Epoch 4, Batch: 120, Loss: 0.085681
    Train - Epoch 4, Batch: 130, Loss: 0.053270
    Train - Epoch 4, Batch: 140, Loss: 0.057205
    Train - Epoch 4, Batch: 150, Loss: 0.057303
    Train - Epoch 4, Batch: 160, Loss: 0.083671
    Train - Epoch 4, Batch: 170, Loss: 0.042737
    Train - Epoch 4, Batch: 180, Loss: 0.024824
    Train - Epoch 4, Batch: 190, Loss: 0.047532
    Train - Epoch 4, Batch: 200, Loss: 0.047744
    Train - Epoch 4, Batch: 210, Loss: 0.028160
    Train - Epoch 4, Batch: 220, Loss: 0.114140
    Train - Epoch 4, Batch: 230, Loss: 0.032848
    Test Avg. Loss: 0.000043, Accuracy: 0.985300
    Train - Epoch 5, Batch: 0, Loss: 0.024736
    Train - Epoch 5, Batch: 10, Loss: 0.047569
    Train - Epoch 5, Batch: 20, Loss: 0.077371
    Train - Epoch 5, Batch: 30, Loss: 0.039319
    Train - Epoch 5, Batch: 40, Loss: 0.009624
    Train - Epoch 5, Batch: 50, Loss: 0.036165
    Train - Epoch 5, Batch: 60, Loss: 0.027070
    Train - Epoch 5, Batch: 70, Loss: 0.028656
    Train - Epoch 5, Batch: 80, Loss: 0.049659
    Train - Epoch 5, Batch: 90, Loss: 0.092047
    Train - Epoch 5, Batch: 100, Loss: 0.040482
    Train - Epoch 5, Batch: 110, Loss: 0.032466
    Train - Epoch 5, Batch: 120, Loss: 0.021912
    Train - Epoch 5, Batch: 130, Loss: 0.018135
    Train - Epoch 5, Batch: 140, Loss: 0.046672
    Train - Epoch 5, Batch: 150, Loss: 0.031142
    Train - Epoch 5, Batch: 160, Loss: 0.065423
    Train - Epoch 5, Batch: 170, Loss: 0.076633
    Train - Epoch 5, Batch: 180, Loss: 0.038218
    Train - Epoch 5, Batch: 190, Loss: 0.024578
    Train - Epoch 5, Batch: 200, Loss: 0.044397
    Train - Epoch 5, Batch: 210, Loss: 0.061336
    Train - Epoch 5, Batch: 220, Loss: 0.030748
    Train - Epoch 5, Batch: 230, Loss: 0.026420
    Test Avg. Loss: 0.000039, Accuracy: 0.987100
    Train - Epoch 6, Batch: 0, Loss: 0.044141
    Train - Epoch 6, Batch: 10, Loss: 0.040236
    Train - Epoch 6, Batch: 20, Loss: 0.048378
    Train - Epoch 6, Batch: 30, Loss: 0.024512
    Train - Epoch 6, Batch: 40, Loss: 0.028769
    Train - Epoch 6, Batch: 50, Loss: 0.034363
    Train - Epoch 6, Batch: 60, Loss: 0.049556
    Train - Epoch 6, Batch: 70, Loss: 0.030666
    Train - Epoch 6, Batch: 80, Loss: 0.053802
    Train - Epoch 6, Batch: 90, Loss: 0.083834
    Train - Epoch 6, Batch: 100, Loss: 0.039355
    Train - Epoch 6, Batch: 110, Loss: 0.050084
    Train - Epoch 6, Batch: 120, Loss: 0.035384
    Train - Epoch 6, Batch: 130, Loss: 0.075478
    Train - Epoch 6, Batch: 140, Loss: 0.061034
    Train - Epoch 6, Batch: 150, Loss: 0.017920
    Train - Epoch 6, Batch: 160, Loss: 0.019840
    Train - Epoch 6, Batch: 170, Loss: 0.066182
    Train - Epoch 6, Batch: 180, Loss: 0.023204
    Train - Epoch 6, Batch: 190, Loss: 0.030185
    Train - Epoch 6, Batch: 200, Loss: 0.015153
    Train - Epoch 6, Batch: 210, Loss: 0.040058
    Train - Epoch 6, Batch: 220, Loss: 0.026025
    Train - Epoch 6, Batch: 230, Loss: 0.025254
    Test Avg. Loss: 0.000033, Accuracy: 0.988200
    Train - Epoch 7, Batch: 0, Loss: 0.024070
    Train - Epoch 7, Batch: 10, Loss: 0.024512
    Train - Epoch 7, Batch: 20, Loss: 0.015800
    Train - Epoch 7, Batch: 30, Loss: 0.021826
    Train - Epoch 7, Batch: 40, Loss: 0.035038
    Train - Epoch 7, Batch: 50, Loss: 0.065198
    Train - Epoch 7, Batch: 60, Loss: 0.018621
    Train - Epoch 7, Batch: 70, Loss: 0.053762
    Train - Epoch 7, Batch: 80, Loss: 0.093103
    Train - Epoch 7, Batch: 90, Loss: 0.012917
    Train - Epoch 7, Batch: 100, Loss: 0.038949
    Train - Epoch 7, Batch: 110, Loss: 0.031542
    Train - Epoch 7, Batch: 120, Loss: 0.016538
    Train - Epoch 7, Batch: 130, Loss: 0.065651
    Train - Epoch 7, Batch: 140, Loss: 0.010077
    Train - Epoch 7, Batch: 150, Loss: 0.029399
    Train - Epoch 7, Batch: 160, Loss: 0.034463
    Train - Epoch 7, Batch: 170, Loss: 0.025253
    Train - Epoch 7, Batch: 180, Loss: 0.026345
    Train - Epoch 7, Batch: 190, Loss: 0.039458
    Train - Epoch 7, Batch: 200, Loss: 0.038242
    Train - Epoch 7, Batch: 210, Loss: 0.019514
    Train - Epoch 7, Batch: 220, Loss: 0.030997
    Train - Epoch 7, Batch: 230, Loss: 0.029049
    Test Avg. Loss: 0.000033, Accuracy: 0.989000
    Train - Epoch 8, Batch: 0, Loss: 0.008282
    Train - Epoch 8, Batch: 10, Loss: 0.017794
    Train - Epoch 8, Batch: 20, Loss: 0.033910
    Train - Epoch 8, Batch: 30, Loss: 0.025411
    Train - Epoch 8, Batch: 40, Loss: 0.015167
    Train - Epoch 8, Batch: 50, Loss: 0.021764
    Train - Epoch 8, Batch: 60, Loss: 0.025642
    Train - Epoch 8, Batch: 70, Loss: 0.026746
    Train - Epoch 8, Batch: 80, Loss: 0.083921
    Train - Epoch 8, Batch: 90, Loss: 0.029759
    Train - Epoch 8, Batch: 100, Loss: 0.037689
    Train - Epoch 8, Batch: 110, Loss: 0.039460
    Train - Epoch 8, Batch: 120, Loss: 0.017434
    Train - Epoch 8, Batch: 130, Loss: 0.034581
    Train - Epoch 8, Batch: 140, Loss: 0.063059
    Train - Epoch 8, Batch: 150, Loss: 0.066770
    Train - Epoch 8, Batch: 160, Loss: 0.025287
    Train - Epoch 8, Batch: 170, Loss: 0.015583
    Train - Epoch 8, Batch: 180, Loss: 0.054220
    Train - Epoch 8, Batch: 190, Loss: 0.029341
    Train - Epoch 8, Batch: 200, Loss: 0.030160
    Train - Epoch 8, Batch: 210, Loss: 0.069261
    Train - Epoch 8, Batch: 220, Loss: 0.016770
    Train - Epoch 8, Batch: 230, Loss: 0.026243
    Test Avg. Loss: 0.000032, Accuracy: 0.988500
    Train - Epoch 9, Batch: 0, Loss: 0.025416
    Train - Epoch 9, Batch: 10, Loss: 0.006188
    Train - Epoch 9, Batch: 20, Loss: 0.020386
    Train - Epoch 9, Batch: 30, Loss: 0.018133
    Train - Epoch 9, Batch: 40, Loss: 0.003498
    Train - Epoch 9, Batch: 50, Loss: 0.023414
    Train - Epoch 9, Batch: 60, Loss: 0.068004
    Train - Epoch 9, Batch: 70, Loss: 0.024850
    Train - Epoch 9, Batch: 80, Loss: 0.015137
    Train - Epoch 9, Batch: 90, Loss: 0.023843
    Train - Epoch 9, Batch: 100, Loss: 0.019276
    Train - Epoch 9, Batch: 110, Loss: 0.020706
    Train - Epoch 9, Batch: 120, Loss: 0.017100
    Train - Epoch 9, Batch: 130, Loss: 0.032251
    Train - Epoch 9, Batch: 140, Loss: 0.006080
    Train - Epoch 9, Batch: 150, Loss: 0.022148
    Train - Epoch 9, Batch: 160, Loss: 0.031210
    Train - Epoch 9, Batch: 170, Loss: 0.002495
    Train - Epoch 9, Batch: 180, Loss: 0.033352
    Train - Epoch 9, Batch: 190, Loss: 0.029081
    Train - Epoch 9, Batch: 200, Loss: 0.007669
    Train - Epoch 9, Batch: 210, Loss: 0.007551
    Train - Epoch 9, Batch: 220, Loss: 0.015849
    Train - Epoch 9, Batch: 230, Loss: 0.020666
    Test Avg. Loss: 0.000036, Accuracy: 0.988300
    Train - Epoch 10, Batch: 0, Loss: 0.010589
    Train - Epoch 10, Batch: 10, Loss: 0.018612
    Train - Epoch 10, Batch: 20, Loss: 0.017199
    Train - Epoch 10, Batch: 30, Loss: 0.010294
    Train - Epoch 10, Batch: 40, Loss: 0.028546
    Train - Epoch 10, Batch: 50, Loss: 0.013994
    Train - Epoch 10, Batch: 60, Loss: 0.008827
    Train - Epoch 10, Batch: 70, Loss: 0.016255
    Train - Epoch 10, Batch: 80, Loss: 0.009431
    Train - Epoch 10, Batch: 90, Loss: 0.014594
    Train - Epoch 10, Batch: 100, Loss: 0.010329
    Train - Epoch 10, Batch: 110, Loss: 0.019751
    Train - Epoch 10, Batch: 120, Loss: 0.019126
    Train - Epoch 10, Batch: 130, Loss: 0.006064
    Train - Epoch 10, Batch: 140, Loss: 0.027301
    Train - Epoch 10, Batch: 150, Loss: 0.043245
    Train - Epoch 10, Batch: 160, Loss: 0.044680
    Train - Epoch 10, Batch: 170, Loss: 0.006976
    Train - Epoch 10, Batch: 180, Loss: 0.034920
    Train - Epoch 10, Batch: 190, Loss: 0.023426
    Train - Epoch 10, Batch: 200, Loss: 0.036866
    Train - Epoch 10, Batch: 210, Loss: 0.008415
    Train - Epoch 10, Batch: 220, Loss: 0.044059
    Train - Epoch 10, Batch: 230, Loss: 0.031490
    Test Avg. Loss: 0.000036, Accuracy: 0.987700
    Train - Epoch 11, Batch: 0, Loss: 0.020564
    Train - Epoch 11, Batch: 10, Loss: 0.011271
    Train - Epoch 11, Batch: 20, Loss: 0.049623
    Train - Epoch 11, Batch: 30, Loss: 0.028951
    Train - Epoch 11, Batch: 40, Loss: 0.014797
    Train - Epoch 11, Batch: 50, Loss: 0.006550
    Train - Epoch 11, Batch: 60, Loss: 0.013403
    Train - Epoch 11, Batch: 70, Loss: 0.011401
    Train - Epoch 11, Batch: 80, Loss: 0.020940
    Train - Epoch 11, Batch: 90, Loss: 0.013957
    Train - Epoch 11, Batch: 100, Loss: 0.001513
    Train - Epoch 11, Batch: 110, Loss: 0.009929
    Train - Epoch 11, Batch: 120, Loss: 0.010340
    Train - Epoch 11, Batch: 130, Loss: 0.042369
    Train - Epoch 11, Batch: 140, Loss: 0.005425
    Train - Epoch 11, Batch: 150, Loss: 0.009119
    Train - Epoch 11, Batch: 160, Loss: 0.038966
    Train - Epoch 11, Batch: 170, Loss: 0.003163
    Train - Epoch 11, Batch: 180, Loss: 0.017247
    Train - Epoch 11, Batch: 190, Loss: 0.037625
    Train - Epoch 11, Batch: 200, Loss: 0.086759
    Train - Epoch 11, Batch: 210, Loss: 0.030909
    Train - Epoch 11, Batch: 220, Loss: 0.032017
    Train - Epoch 11, Batch: 230, Loss: 0.048916
    Test Avg. Loss: 0.000033, Accuracy: 0.989800
    Train - Epoch 12, Batch: 0, Loss: 0.027991
    Train - Epoch 12, Batch: 10, Loss: 0.007180
    Train - Epoch 12, Batch: 20, Loss: 0.011618
    Train - Epoch 12, Batch: 30, Loss: 0.009657
    Train - Epoch 12, Batch: 40, Loss: 0.003374
    Train - Epoch 12, Batch: 50, Loss: 0.019330
    Train - Epoch 12, Batch: 60, Loss: 0.015131
    Train - Epoch 12, Batch: 70, Loss: 0.040882
    Train - Epoch 12, Batch: 80, Loss: 0.008052
    Train - Epoch 12, Batch: 90, Loss: 0.006585
    Train - Epoch 12, Batch: 100, Loss: 0.028739
    Train - Epoch 12, Batch: 110, Loss: 0.009761
    Train - Epoch 12, Batch: 120, Loss: 0.019616
    Train - Epoch 12, Batch: 130, Loss: 0.011868
    Train - Epoch 12, Batch: 140, Loss: 0.009689
    Train - Epoch 12, Batch: 150, Loss: 0.005598
    Train - Epoch 12, Batch: 160, Loss: 0.004077
    Train - Epoch 12, Batch: 170, Loss: 0.015676
    Train - Epoch 12, Batch: 180, Loss: 0.014377
    Train - Epoch 12, Batch: 190, Loss: 0.004340
    Train - Epoch 12, Batch: 200, Loss: 0.038935
    Train - Epoch 12, Batch: 210, Loss: 0.035879
    Train - Epoch 12, Batch: 220, Loss: 0.024816
    Train - Epoch 12, Batch: 230, Loss: 0.012925
    Test Avg. Loss: 0.000033, Accuracy: 0.989100
    Train - Epoch 13, Batch: 0, Loss: 0.022052
    Train - Epoch 13, Batch: 10, Loss: 0.012797
    Train - Epoch 13, Batch: 20, Loss: 0.013350
    Train - Epoch 13, Batch: 30, Loss: 0.023859
    Train - Epoch 13, Batch: 40, Loss: 0.033041
    Train - Epoch 13, Batch: 50, Loss: 0.037284
    Train - Epoch 13, Batch: 60, Loss: 0.019953
    Train - Epoch 13, Batch: 70, Loss: 0.003509
    Train - Epoch 13, Batch: 80, Loss: 0.005512
    Train - Epoch 13, Batch: 90, Loss: 0.010827
    Train - Epoch 13, Batch: 100, Loss: 0.013831
    Train - Epoch 13, Batch: 110, Loss: 0.014214
    Train - Epoch 13, Batch: 120, Loss: 0.015369
    Train - Epoch 13, Batch: 130, Loss: 0.024683
    Train - Epoch 13, Batch: 140, Loss: 0.029714
    Train - Epoch 13, Batch: 150, Loss: 0.003146
    Train - Epoch 13, Batch: 160, Loss: 0.019133
    Train - Epoch 13, Batch: 170, Loss: 0.016122
    Train - Epoch 13, Batch: 180, Loss: 0.016242
    Train - Epoch 13, Batch: 190, Loss: 0.049664
    Train - Epoch 13, Batch: 200, Loss: 0.015133
    Train - Epoch 13, Batch: 210, Loss: 0.028728
    Train - Epoch 13, Batch: 220, Loss: 0.026338
    Train - Epoch 13, Batch: 230, Loss: 0.017409
    Test Avg. Loss: 0.000025, Accuracy: 0.992200
    Train - Epoch 14, Batch: 0, Loss: 0.003295
    Train - Epoch 14, Batch: 10, Loss: 0.034162
    Train - Epoch 14, Batch: 20, Loss: 0.009937
    Train - Epoch 14, Batch: 30, Loss: 0.005851
    Train - Epoch 14, Batch: 40, Loss: 0.016461
    Train - Epoch 14, Batch: 50, Loss: 0.015125
    Train - Epoch 14, Batch: 60, Loss: 0.017636
    Train - Epoch 14, Batch: 70, Loss: 0.035415
    Train - Epoch 14, Batch: 80, Loss: 0.026546
    Train - Epoch 14, Batch: 90, Loss: 0.029204
    Train - Epoch 14, Batch: 100, Loss: 0.022895
    Train - Epoch 14, Batch: 110, Loss: 0.020067
    Train - Epoch 14, Batch: 120, Loss: 0.004506
    Train - Epoch 14, Batch: 130, Loss: 0.008805
    Train - Epoch 14, Batch: 140, Loss: 0.007999
    Train - Epoch 14, Batch: 150, Loss: 0.003335
    Train - Epoch 14, Batch: 160, Loss: 0.004733
    Train - Epoch 14, Batch: 170, Loss: 0.009495
    Train - Epoch 14, Batch: 180, Loss: 0.006398
    Train - Epoch 14, Batch: 190, Loss: 0.009070
    Train - Epoch 14, Batch: 200, Loss: 0.006565
    Train - Epoch 14, Batch: 210, Loss: 0.020091
    Train - Epoch 14, Batch: 220, Loss: 0.013111
    Train - Epoch 14, Batch: 230, Loss: 0.019667
    Test Avg. Loss: 0.000035, Accuracy: 0.989700
    Train - Epoch 15, Batch: 0, Loss: 0.015832
    Train - Epoch 15, Batch: 10, Loss: 0.000755
    Train - Epoch 15, Batch: 20, Loss: 0.022132
    Train - Epoch 15, Batch: 30, Loss: 0.026901
    Train - Epoch 15, Batch: 40, Loss: 0.007520
    Train - Epoch 15, Batch: 50, Loss: 0.003284
    Train - Epoch 15, Batch: 60, Loss: 0.022684
    Train - Epoch 15, Batch: 70, Loss: 0.004368
    Train - Epoch 15, Batch: 80, Loss: 0.015036
    Train - Epoch 15, Batch: 90, Loss: 0.008023
    Train - Epoch 15, Batch: 100, Loss: 0.021108
    Train - Epoch 15, Batch: 110, Loss: 0.027633
    Train - Epoch 15, Batch: 120, Loss: 0.009627
    Train - Epoch 15, Batch: 130, Loss: 0.004858
    Train - Epoch 15, Batch: 140, Loss: 0.010220
    Train - Epoch 15, Batch: 150, Loss: 0.005526
    Train - Epoch 15, Batch: 160, Loss: 0.005549
    Train - Epoch 15, Batch: 170, Loss: 0.017708
    Train - Epoch 15, Batch: 180, Loss: 0.008478
    Train - Epoch 15, Batch: 190, Loss: 0.002881
    Train - Epoch 15, Batch: 200, Loss: 0.008359
    Train - Epoch 15, Batch: 210, Loss: 0.019088
    Train - Epoch 15, Batch: 220, Loss: 0.011107
    Train - Epoch 15, Batch: 230, Loss: 0.005971
    Test Avg. Loss: 0.000034, Accuracy: 0.990200



```python
import matplotlib.pyplot as plt
```


```python
plt.figure(figsize=(20, 10))
plt.plot(train_loss)
```




    [<matplotlib.lines.Line2D at 0x7fe88e106b38>]




    
![png](LeNet-5_files/LeNet-5_16_1.png)
    



```python

```
