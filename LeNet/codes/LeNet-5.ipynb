{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.13 64-bit ('python3.6env': conda)"
  },
  "interpreter": {
   "hash": "03c9982e4e5e5aafe188fcddaa066fbf54983174d73da9138b399c741ef8645a"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## 定义LeNet-5网络"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# LeNet-5网络仿真实现\n",
    "\n",
    "## LeNet-5网络架构介绍\n",
    "\n",
    "`LeNet-5`网络架构如下图所示，不包括输入层的话，`LeNet-5`一共有7层，所有的层都有可以训练的参数(权重).\n",
    "![](../imgs/lenet5.png)\n",
    "输入的图像是  $32\\times32$ 尺寸的点阵图，这比用于训练的数据库中的最大的字符还要大(数据库中的大多数数据尺寸在 $20\\times20$​​​​)。这样做的原因是期望识别到潜在的区别特征，例如笔划终点或转角可以出现在最高水平特征检测器的感受野的中心。在 $32\\times32$​的输入数据中，LeNet-5网络的最后一层卷积层的感受野中心形成 $20\\times20$​的区域。\n",
    "\n",
    "输入的数据经过归一化，白色的点在-0.1，黑色的点在1.175，这样让输入的平均值在0左右，方差在1左右，可以加速学习。\n",
    "\n",
    "\n",
    "这个网络架构虽然比较小，但是也包含了深度学习的主要的基本模块：\n",
    "\n",
    "### input layer\n",
    "\n",
    "数据输入层，将输入图像尺寸统一并归一化为 $32\\times32$\n",
    "\n",
    "### c1卷积层(convolutional layer)\n",
    "\n",
    "* 输入：$32 \\times 32$\n",
    "\n",
    "* 卷积核大小：$5\\times5, s=1$​\n",
    "\n",
    "* 卷积种类：6\n",
    "\n",
    "* 神经元数量：$28\\times28\\times6$\n",
    "\n",
    "* 可训练参数：$(5\\times5+1)\\times6=156$​ \n",
    "\n",
    "  > 每个滤波器 $5\\times5=25$​个`unit`参数和一个`bias`参数，一共6个滤波器\n",
    "\n",
    "* 连接数：$(5\\times5+1)\\times6\\times28\\times28=122304$​\n",
    "\n",
    "  > 卷积层`C1`内的每个像素都与输入图像中的 $5\\times5$​个像素和1个bias有连接，所以总共有 $156\\times28\\times28=12304$​ 个连接点\n",
    "\n",
    "> 对输入图像进行第一次卷积运算（使用 6 个大小为 $5\\times5$​​​​ 的卷积核），得到6个`C1`特征图（6个大小为$28\\times28$​​的 `feature maps`, 32-5+1=28）。我们再来看看需要多少个参数，卷积核的大小为$5\\times5$​​，总共就有$6\\times(5\\times5+1)=156$​​​个参数，其中+1是表示一个核有一个`bias`。对于卷积层`C1`，`C1`内的每个像素都与输入图像中的$5\\times5$​​个像素和1个`bias`有连接，所以总共有$156\\times28\\times28=122304$​​​个连接（connection）。有122304个连接，但是我们只需要学习156个参数，主要是通过权值共享实现的。\n",
    "\n",
    "### S2池化层(sub-sampS2ling layer)\n",
    "\n",
    "* 输入：$28\\times28$\n",
    "* 采样区域：$2\\times2$\n",
    "* 采样方式：4个输入相加，乘以一个可训练参数，再加上一个可训练偏置。结果通过`sigmoid`函数\n",
    "* 采样种类：6\n",
    "* 输出`featureMap`大小：$14\\times14(28/2)$​\n",
    "* 神经元数量：$14\\times14\\times6$​\n",
    "* 可训练参数：$2\\times6$​(和的权+偏置)\n",
    "* 连接数：$(2\\times2+1)\\times6\\times14\\times14$​\n",
    "* `S2`中每个特征图的大小是`C1`中特征图大小的1/4。\n",
    "\n",
    "> 第一次卷积之后紧接着就是池化运算，使用 $2\\times 2$​​​核进行池化，于是得到了`S2`，6个$14\\times14$​​​​的特征图（28/2=14）。`S2`这个pooling层是对C1中的 $2\\times2$​​​​ 区域内的像素求和乘以一个权值系数再加上一个偏置，然后将这个结果再做一次映射。于是每个池化核有两个训练参数，所以共有2x6=12个训练参数，但是有5x14x14x6=5880个连接。\n",
    "\n",
    "### C3卷积层(convolutional layer)\n",
    "\n",
    "* `S2`中所有6个或者几个特征`map`组合\n",
    "* 卷积核大小：$5\\times5$\n",
    "* 卷积核种类：16\n",
    "* 输出`featureMap`大小：$10\\times10(14-5+1)=10$\n",
    "\n",
    "![](../imgs/table1.png)\n",
    "\n",
    "`C3`中的每个特征`map`是连接到`S2`中的所有6个或者几个特征`map`的，表示本层的特征`map`是上一层提取到的特征`map`的不同组合。存在的一个方式是：`C3`的前6个特征图以`S2`中3个相邻的特征图子集为输入。接下来6个特征图以`S2`中4个相邻特征图子集为输入。然后的3个以不相邻的4个特征图子集为输入。最后一个将`S2`中所有特征图为输入。则：可训练参数：\n",
    "$$\n",
    "6\\times(3\\times5\\times5+1)+6\\times(4\\times5\\times5+1)+3\\times(4\\times5\\times5+1)+\\times(6\\times5\\times5+1)=1516\n",
    "$$\n",
    "这种非对称的连接的作用是：\n",
    "\n",
    "* 非完全连接的方案可以使连接数保持在合理的范围内\n",
    "* 不同的特征图因为有不同的特征可以提取多种组合\n",
    "\n",
    "### S4池化层(sub-sampling layer)\n",
    "\n",
    "* 输入：$10\\times10$\n",
    "* 采样区域：$2\\times2$\n",
    "* 采样方式：4个输入相加，乘以一个可训练参数，再加上一个可训练偏置。结果通过`sigmoid`\n",
    "* 采样种类：16\n",
    "* 输出`featureMap`大小：$5\\times5$\n",
    "* 神经元数量：$5\\times5\\times16=400$\n",
    "* 可训练参数：$2\\times16=32$\n",
    "* 连接数：$16\\times(2\\times2+1)\\times5\\times5=2000$​\n",
    "* `S4`中每个特征图的大小是`C3`中特征图大小的1/4\n",
    "\n",
    "> `S4`是`pooling`层，窗口大小仍然是$2\\times2$​​​，共计16个`feature map`，`C3`层的16个$10\\times10$​​​的图分别进行以$2\\times2$​​​为单位的池化得到16个 $5\\times5$​​​ 的特征图。这一层有2x16共32个训练参数，$5\\times5\\times5\\times16=2000$​​ 个连接。连接的方式与`S2`层类似。\n",
    "\n",
    "### C5卷积层(convolutional layer)\n",
    "\n",
    "* 输入：`S4`层的全部16个单元`featureMap`(与`S4`全相连)\n",
    "* 卷积核大小：$5\\times5$\n",
    "* 卷积核种类：120\n",
    "* 输出`featureMap`大小：$1\\times1(5-5+1)$\n",
    "* 可训练参数/连接：$120\\times(16\\times5\\times5+1)=48120$\n",
    "\n",
    "> `C5`层是一个有120个`featureMap`的卷积层。每一个单元和`S4`的所有16个`featureMap`的 $5\\times 5$​ 邻域全连接。因为`S4`的`featureMap`是 $5 \\times 5$ ，所以`C5`的`featureMap`是 $1\\times1$ 。\n",
    "\n",
    "### F6全连接层(fully-ocnnected layer)\n",
    "\n",
    "* 输入：`C5` 120维向量\n",
    "* 计算方式：计算输入向量和权重向量之间的点积，再加上一个偏置，结果通过`sigmoid`函数输出。\n",
    "* 可训练参数：$84\\times(120+1)=10164$\n",
    "\n",
    "包含84个节点，对应于一个 $7\\times12$ 的比特图，$-1$ 表示白色， $1$​ 表示黑色，这样每个符号的比特图的黑白色就对应于一个编码。\n",
    "\n",
    "![](../imgs/fig3.png)\n",
    "\n",
    "### output layer\n",
    "\n",
    "最后，输出层由欧几里得径向基函数(Euclidean Radial Basis Function)组成，也是全连接层。共有10个节点，分别表示0到9，对于每个`RBF`单元$y_i$的输出：\n",
    "$$\n",
    "y_i = \\sum_j(x_j-w_{ij})^2\n",
    "$$\n",
    "\n",
    "### 总结\n",
    "\n",
    "现在的大多数神经网络是通过`softmax`函数输出多分类结果，相比于现代版本，这里得到的神经网络会小一些，只有约6万个参数，现在的一些神经网络甚至有一千万到一亿个参数。\n",
    "\n",
    "从`LeNet-5`网络从左往右看，随着网络越来越深，图像的高度和宽度都在缩小，从最初的 $32\\times32$​ 缩小到 $28\\times28$​，再到 $14\\times14$​、$10\\times10$​，最后只用 $5\\times5$​，与此同时，随着网络层次的加深，通道数量一直在增加，从1增加到6个，再到16个。\n",
    "\n",
    "LeNet5网络的特别之处还在于，各个网络之间是有关联的，比如说，你有一个$nH\\times nW\\times nC$​​ 的网络，有$nC$​​个通道，使用尺寸为$𝑓 × 𝑓 × 𝑛 𝐶 $​的过滤器，每个过滤器的通道数和它上一层的通道数相同。这是由于在当时，计算机的运行速度非常慢，为了减少计算量和参数，经典的 `LeNet-5 `网络使用了非常复杂的计算方式，每个过滤器都采用和输入模块一样的通道数量。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## LeNet-5网络定义"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "from collections import OrderedDict\n",
    "import torch.nn as nn\n",
    "class C1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(C1, self).__init__()\n",
    "\n",
    "        self.c1 = nn.Sequential(OrderedDict([\n",
    "            ('c1', nn.Conv2d(1, 6, kernel_size=(5, 5))),\n",
    "            ('relu1', nn.ReLU()),\n",
    "            ('s1', nn.MaxPool2d(kernel_size=(2, 2), stride=2))\n",
    "        ]))\n",
    "\n",
    "    def forward(self, img):\n",
    "        output = self.c1(img)\n",
    "        return output\n",
    "\n",
    "\n",
    "class C2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(C2, self).__init__()\n",
    "\n",
    "        self.c2 = nn.Sequential(OrderedDict([\n",
    "            ('c2', nn.Conv2d(6, 16, kernel_size=(5, 5))),\n",
    "            ('relu2', nn.ReLU()),\n",
    "            ('s2', nn.MaxPool2d(kernel_size=(2, 2), stride=2))\n",
    "        ]))\n",
    "\n",
    "    def forward(self, img):\n",
    "        output = self.c2(img)\n",
    "        return output\n",
    "\n",
    "\n",
    "class C3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(C3, self).__init__()\n",
    "\n",
    "        self.c3 = nn.Sequential(OrderedDict([\n",
    "            ('c3', nn.Conv2d(16, 120, kernel_size=(5, 5))),\n",
    "            ('relu3', nn.ReLU())\n",
    "        ]))\n",
    "\n",
    "    def forward(self, img):\n",
    "        output = self.c3(img)\n",
    "        return output\n",
    "\n",
    "\n",
    "class F4(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(F4, self).__init__()\n",
    "\n",
    "        self.f4 = nn.Sequential(OrderedDict([\n",
    "            ('f4', nn.Linear(120, 84)),\n",
    "            ('relu4', nn.ReLU())\n",
    "        ]))\n",
    "\n",
    "    def forward(self, img):\n",
    "        output = self.f4(img)\n",
    "        return output\n",
    "\n",
    "\n",
    "class F5(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(F5, self).__init__()\n",
    "\n",
    "        self.f5 = nn.Sequential(OrderedDict([\n",
    "            ('f5', nn.Linear(84, 10)),\n",
    "            ('sig5', nn.LogSoftmax(dim=-1))\n",
    "        ]))\n",
    "\n",
    "    def forward(self, img):\n",
    "        output = self.f5(img)\n",
    "        return output\n",
    "\n",
    "\n",
    "class LeNet5(nn.Module):\n",
    "    \"\"\"\n",
    "    Input - 1x32x32\n",
    "    Output - 10\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(LeNet5, self).__init__()\n",
    "\n",
    "        self.c1 = C1()\n",
    "        self.c2_1 = C2() \n",
    "        self.c2_2 = C2() \n",
    "        self.c3 = C3() \n",
    "        self.f4 = F4() \n",
    "        self.f5 = F5() \n",
    "\n",
    "    def forward(self, img):\n",
    "        output = self.c1(img)\n",
    "\n",
    "        x = self.c2_1(output)\n",
    "        output = self.c2_2(output)\n",
    "\n",
    "        output += x\n",
    "\n",
    "        output = self.c3(output)\n",
    "        output = output.view(img.size(0), -1)\n",
    "        output = self.f4(output)\n",
    "        output = self.f5(output)\n",
    "        return output\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 数据准备\n",
    "\n",
    "使用的数据是MNIST的手写字符集，可以直接通过`torchvision.datasets.mnist`获取"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "from torchvision.datasets.mnist import MNIST\n",
    "from lenet import LeNet5\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torchvision.datasets.mnist import MNIST\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import visdom\n",
    "import onnx\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "viz = visdom.Visdom()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Setting up a new session...\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "data_train = MNIST('./data/mnist', \n",
    "                    download=True,\n",
    "                    transform=transforms.transforms.Compose([\n",
    "                        transforms.Resize((32, 32)), \n",
    "                        transforms.transforms.ToTensor()\n",
    "                    ]))\n",
    "data_test = MNIST('./data/mnist',\n",
    "                    train=False,\n",
    "                    download=True,\n",
    "                    transform=transforms.transforms.Compose([\n",
    "                        transforms.Resize((32, 32)),\n",
    "                        transforms.ToTensor()])\n",
    "                    )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "source": [
    "data_train_loader = DataLoader(data_train, batch_size=256, shuffle=True, num_workers=8)\n",
    "data_test_loader = DataLoader(data_test, batch_size=1024, num_workers=8)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "source": [
    "net = LeNet5().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=2e-3)\n",
    "cur_batch_win = None\n",
    "cur_batch_win_opts = {\n",
    "    'title': 'Epoch Loss Trace',\n",
    "    'xlabel': 'Batch Number',\n",
    "    'ylabel': 'Loss',\n",
    "    'width': 1200,\n",
    "    'height': 600,\n",
    "}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "source": [
    "def train(epoch):\n",
    "    global cur_batch_win\n",
    "    net.train()\n",
    "    loss_list, batch_list = [], []\n",
    "    for i, (images, labels) in enumerate(data_train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        output = net(images)\n",
    "\n",
    "        loss = criterion(output, labels)\n",
    "\n",
    "        loss_list.append(loss.detach().cpu())\n",
    "        batch_list.append(i+1)\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print('Train - Epoch %d, Batch: %d, Loss: %f' % (epoch, i, loss.detach().cpu().item()))\n",
    "\n",
    "        # Update Visualization\n",
    "        if viz.check_connection():\n",
    "            cur_batch_win = viz.line(torch.Tensor(loss_list), torch.Tensor(batch_list),\n",
    "                                     win=cur_batch_win, name='current_batch_loss'+str(epoch),\n",
    "                                     update=(None if cur_batch_win is None else 'replace'),\n",
    "                                     opts=cur_batch_win_opts)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "def test():\n",
    "    net.eval()\n",
    "    total_correct = 0\n",
    "    avg_loss = 0.0\n",
    "\n",
    "    for i, (images, labels) in enumerate(data_test_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        output = net(images)\n",
    "\n",
    "        avg_loss += criterion(output, labels).sum()\n",
    "        pred = output.detach().cpu().max(1)[1]\n",
    "        total_correct += pred.eq(labels.cpu().view_as(pred)).sum()\n",
    "\n",
    "    avg_loss /= len(data_test)\n",
    "    print('Test Avg. Loss: %f, Accuracy: %f' % (avg_loss.detach().cpu().item(), float(total_correct) / len(data_test)))\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "source": [
    "def train_and_test(epoch):\n",
    "    train(epoch)\n",
    "    test()\n",
    "\n",
    "    dummy_input = torch.randn(1, 1, 32, 32, requires_grad=True).to(device)\n",
    "    torch.onnx.export(net, dummy_input, \"lenet.onnx\")\n",
    "\n",
    "    onnx_model = onnx.load(\"lenet.onnx\")\n",
    "    onnx.checker.check_model(onnx_model)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "source": [
    "def main():\n",
    "    for e in range(1, 16):\n",
    "        train_and_test(e)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "source": [
    "main()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train - Epoch 1, Batch: 0, Loss: 2.310019\n",
      "Train - Epoch 1, Batch: 10, Loss: 1.752108\n",
      "Train - Epoch 1, Batch: 20, Loss: 0.929540\n",
      "Train - Epoch 1, Batch: 30, Loss: 0.659155\n",
      "Train - Epoch 1, Batch: 40, Loss: 0.600451\n",
      "Train - Epoch 1, Batch: 50, Loss: 0.525040\n",
      "Train - Epoch 1, Batch: 60, Loss: 0.442140\n",
      "Train - Epoch 1, Batch: 70, Loss: 0.414708\n",
      "Train - Epoch 1, Batch: 80, Loss: 0.305595\n",
      "Train - Epoch 1, Batch: 90, Loss: 0.228443\n",
      "Train - Epoch 1, Batch: 100, Loss: 0.227592\n",
      "Train - Epoch 1, Batch: 110, Loss: 0.297097\n",
      "Train - Epoch 1, Batch: 120, Loss: 0.202451\n",
      "Train - Epoch 1, Batch: 130, Loss: 0.234074\n",
      "Train - Epoch 1, Batch: 140, Loss: 0.124735\n",
      "Train - Epoch 1, Batch: 150, Loss: 0.184467\n",
      "Train - Epoch 1, Batch: 160, Loss: 0.139945\n",
      "Train - Epoch 1, Batch: 170, Loss: 0.145408\n",
      "Train - Epoch 1, Batch: 180, Loss: 0.134222\n",
      "Train - Epoch 1, Batch: 190, Loss: 0.192816\n",
      "Train - Epoch 1, Batch: 200, Loss: 0.154474\n",
      "Train - Epoch 1, Batch: 210, Loss: 0.135778\n",
      "Train - Epoch 1, Batch: 220, Loss: 0.141124\n",
      "Train - Epoch 1, Batch: 230, Loss: 0.160964\n",
      "Test Avg. Loss: 0.000118, Accuracy: 0.964000\n",
      "Train - Epoch 2, Batch: 0, Loss: 0.187966\n",
      "Train - Epoch 2, Batch: 10, Loss: 0.189266\n",
      "Train - Epoch 2, Batch: 20, Loss: 0.094599\n",
      "Train - Epoch 2, Batch: 30, Loss: 0.092561\n",
      "Train - Epoch 2, Batch: 40, Loss: 0.100277\n",
      "Train - Epoch 2, Batch: 50, Loss: 0.166138\n",
      "Train - Epoch 2, Batch: 60, Loss: 0.133650\n",
      "Train - Epoch 2, Batch: 70, Loss: 0.092836\n",
      "Train - Epoch 2, Batch: 80, Loss: 0.082826\n",
      "Train - Epoch 2, Batch: 90, Loss: 0.067225\n",
      "Train - Epoch 2, Batch: 100, Loss: 0.063126\n",
      "Train - Epoch 2, Batch: 110, Loss: 0.084583\n",
      "Train - Epoch 2, Batch: 120, Loss: 0.086398\n",
      "Train - Epoch 2, Batch: 130, Loss: 0.144487\n",
      "Train - Epoch 2, Batch: 140, Loss: 0.183461\n",
      "Train - Epoch 2, Batch: 150, Loss: 0.089760\n",
      "Train - Epoch 2, Batch: 160, Loss: 0.080518\n",
      "Train - Epoch 2, Batch: 170, Loss: 0.132796\n",
      "Train - Epoch 2, Batch: 180, Loss: 0.122537\n",
      "Train - Epoch 2, Batch: 190, Loss: 0.081076\n",
      "Train - Epoch 2, Batch: 200, Loss: 0.086551\n",
      "Train - Epoch 2, Batch: 210, Loss: 0.098619\n",
      "Train - Epoch 2, Batch: 220, Loss: 0.106265\n",
      "Train - Epoch 2, Batch: 230, Loss: 0.067322\n",
      "Test Avg. Loss: 0.000065, Accuracy: 0.978500\n",
      "Train - Epoch 3, Batch: 0, Loss: 0.116501\n",
      "Train - Epoch 3, Batch: 10, Loss: 0.057194\n",
      "Train - Epoch 3, Batch: 20, Loss: 0.069856\n",
      "Train - Epoch 3, Batch: 30, Loss: 0.036910\n",
      "Train - Epoch 3, Batch: 40, Loss: 0.038521\n",
      "Train - Epoch 3, Batch: 50, Loss: 0.039087\n",
      "Train - Epoch 3, Batch: 60, Loss: 0.059389\n",
      "Train - Epoch 3, Batch: 70, Loss: 0.071782\n",
      "Train - Epoch 3, Batch: 80, Loss: 0.084211\n",
      "Train - Epoch 3, Batch: 90, Loss: 0.047995\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.plot(train_loss)"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'train_loss' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-c058896c7dba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'train_loss' is not defined"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 1440x720 with 0 Axes>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Connection to remote host was lost.\n",
      "Connection to remote host was lost.\n",
      "Connection to remote host was lost.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ]
}