{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "PlayGround",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.6.2 64-bit ('python3.6env': conda)"
    },
    "accelerator": "GPU",
    "language_info": {
      "name": "python",
      "version": "3.6.2",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "interpreter": {
      "hash": "7835acacaeaf508b921ecd27357232a38d5f7dc52657613320ee88843bc3a5f4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# SeNet-cifar10-pytorch"
      ],
      "metadata": {
        "id": "WF177SqgBIic",
        "colab_type": "code",
        "colab": {}
      }
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "source": [
        "import torch\r\n",
        "from torch import nn, optim\r\n",
        "from torch.autograd import Variable\r\n",
        "import torch.nn.functional as F\r\n",
        "\r\n",
        "from torch.utils.data import DataLoader\r\n",
        "from torchvision.datasets import CIFAR10\r\n",
        "from torchvision import transforms\r\n",
        "import time\r\n",
        "\r\n",
        "device = ('cuda' if torch.cuda.is_available() else 'cpu')\r\n",
        "device"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cpu'"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "source": [
        "# 超参数\r\n",
        "EPOCHS = 40\r\n",
        "BATCH_SIZE = 128\r\n",
        "LEARNING_RATE = 1e-1\r\n",
        "WEIGHT_DECAY = 1e-4"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 获取数据\r\n",
        "\r\n",
        "使用`torchvision.dataset`获取数据"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "source": [
        "train_dataset = CIFAR10(root='./data', train=True, download=True, transform=transforms.Compose([\r\n",
        "        transforms.RandomCrop(32, padding=4),\r\n",
        "        transforms.RandomHorizontalFlip(),\r\n",
        "        transforms.ToTensor(),\r\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\r\n",
        "    ]))\r\n",
        "test_dataset = CIFAR10(root='./data', train=False, download=True, transform=transforms.Compose([\r\n",
        "    transforms.ToTensor(),\r\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\r\n",
        "]))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\r\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 定义SeNet模型"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "source": [
        "# Squeeze and Excitation Block Module\r\n",
        "class SEBlock(nn.Module):\r\n",
        "    def __init__(self, channels, reduction=16):\r\n",
        "        super(SEBlock, self).__init__()\r\n",
        "\r\n",
        "        self.fc = nn.Sequential(\r\n",
        "            nn.Conv2d(channels, channels // reduction, 1, bias=False),\r\n",
        "            nn.ReLU(),\r\n",
        "            nn.Conv2d(channels // reduction, channels * 2, 1, bias=False),\r\n",
        "        )\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        w = F.adaptive_avg_pool2d(x, 1) # Squeeze\r\n",
        "        w = self.fc(x)\r\n",
        "        w, b = w.split(w.data.size(1) // 2, dim=1) # Excitation\r\n",
        "        w = torch.sigmoid(w)\r\n",
        "\r\n",
        "        return x * w + b # Scale and add bias\r\n"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "source": [
        "# Residual Block with SEBlock\r\n",
        "class ResBlock(nn.Module):\r\n",
        "    def __init__(self, channels):\r\n",
        "        super(ResBlock, self).__init__()\r\n",
        "\r\n",
        "        self.conv_lower = nn.Sequential(\r\n",
        "            nn.Conv2d(channels, channels, 3, padding=1, bias=False),\r\n",
        "            nn.BatchNorm2d(channels),\r\n",
        "            nn.ReLU()\r\n",
        "        )\r\n",
        "\r\n",
        "        self.conv_upper = nn.Sequential(\r\n",
        "            nn.Conv2d(channels, channels, 3, padding=1, bias=False),\r\n",
        "            nn.BatchNorm2d(channels)\r\n",
        "        )\r\n",
        "\r\n",
        "        self.se_block = SEBlock(channels)\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        path = self.conv_lower(x)\r\n",
        "        path = self.conv_upper(path)\r\n",
        "\r\n",
        "        path = self.se_block(path)\r\n",
        "\r\n",
        "        path = x + path\r\n",
        "        return F.relu(path)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "source": [
        "# Network Module\r\n",
        "class Network(nn.Module):\r\n",
        "    def __init__(self, in_channel, filters, blocks, num_classes):\r\n",
        "        super(Network, self).__init__()\r\n",
        "\r\n",
        "        self.conv_block = nn.Sequential(\r\n",
        "            nn.Conv2d(in_channel, filters, 3, padding=1, bias=False),\r\n",
        "            nn.BatchNorm2d(filters),\r\n",
        "            nn.ReLU()\r\n",
        "        )\r\n",
        "\r\n",
        "        self.res_blocks = nn.Sequential(*[ResBlock(filters) for _ in range(blocks - 1)])\r\n",
        "\r\n",
        "        self.out_conv = nn.Sequential(\r\n",
        "            nn.Conv2d(filters, 128, 1, padding=0, bias=False),\r\n",
        "            nn.BatchNorm2d(128),\r\n",
        "            nn.ReLU()\r\n",
        "        )\r\n",
        "\r\n",
        "        self.fc = nn.Linear(128, num_classes)\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        x = self.conv_block(x)\r\n",
        "        x = self.res_blocks(x)\r\n",
        "        \r\n",
        "        x = self.out_conv(x)\r\n",
        "        x = F.adaptive_avg_pool2d(x, 1)\r\n",
        "\r\n",
        "        x = x.view(x.data.size(0), -1)\r\n",
        "        x = self.fc(x)\r\n",
        "\r\n",
        "        return F.log_softmax(x, dim=1)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 训练模型 "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "source": [
        "net = Network(3, 128, 10, 10).to(device)\r\n",
        "\r\n",
        "ACE = nn.CrossEntropyLoss().to(device)\r\n",
        "opt = optim.SGD(net.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY, momentum=.9, nesterov=True)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "source": [
        "for epoch in range(1, EPOCHS + 1):\r\n",
        "        print('[Epoch %d]' % epoch)\r\n",
        "        \r\n",
        "        train_loss = 0\r\n",
        "        train_correct, train_total = 0, 0\r\n",
        "\r\n",
        "        start_point = time.time()\r\n",
        "\r\n",
        "        for inputs, labels in train_loader:\r\n",
        "            inputs, labels = Variable(inputs).to(device), Variable(labels).to(device)\r\n",
        "\r\n",
        "            opt.zero_grad()\r\n",
        "\r\n",
        "            preds = net(inputs)\r\n",
        "            \r\n",
        "            loss = ACE(preds, labels)\r\n",
        "            loss.backward()\r\n",
        "\r\n",
        "            opt.step()\r\n",
        "\r\n",
        "            train_loss += loss.item()\r\n",
        "\r\n",
        "            train_correct += (preds.argmax(dim=1) == labels).sum().item()\r\n",
        "            train_total += len(preds)\r\n",
        "\r\n",
        "        print('train-acc : %.4f%% train-loss : %.5f' % (100 * train_correct / train_total, train_loss / len(train_loader)))\r\n",
        "        print('elapsed time: %ds' % (time.time() - start_point))\r\n",
        "\r\n",
        "        test_loss = 0\r\n",
        "        test_correct, test_total = 0, 0\r\n",
        "\r\n",
        "        for inputs, labels in test_loader:\r\n",
        "            with torch.no_grad():\r\n",
        "                inputs, labels = Variable(inputs).to(device), Variable(labels).to(device)\r\n",
        "\r\n",
        "                preds = net(inputs)\r\n",
        "\r\n",
        "                test_loss += ACE(preds, labels).item()\r\n",
        "\r\n",
        "                test_correct += (preds.argmax(dim=1) == labels).sum().item()\r\n",
        "                test_total += len(preds)\r\n",
        "\r\n",
        "        print('test-acc : %.4f%% test-loss : %.5f' % (100 * test_correct / test_total, test_loss / len(test_loader)))\r\n",
        "        \r\n",
        "        torch.save(net.state_dict(), './checkpoint/checkpoint-%04d.bin' % epoch)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 1]\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [],
      "outputs": [],
      "metadata": {}
    }
  ]
}