\section{Introduction}

In the last three years, mainly due to the advances of deep learning, more concretely convolutional networks ~\cite{lecun1989backprop}, the quality of image recognition and object detection has been progressing at a dramatic pace. One encouraging news is that most of this progress is not just the result of more powerful hardware, larger datasets and bigger models, but mainly a consequence of new ideas, algorithms and improved network architectures. No new data sources were used, for example, by the top entries in the ILSVRC 2014 competition besides the classification dataset of the same competition for detection purposes. Our GoogLeNet submission to ILSVRC 2014 actually uses $12\times$ fewer parameters than the winning architecture of Krizhevsky et al~\cite{krizhevsky2012imagenet} from two years ago, while being significantly more accurate. The biggest gains in object-detection have not come from the utilization of deep networks alone or bigger models, but from the synergy of deep architectures and classical computer vision, like the R-CNN algorithm by Girshick et al~\cite{girshick2014rich}. 

Another notable factor is that with the ongoing traction of mobile and embedded computing, the efficiency of our algorithms -- especially their power and memory use -- gains importance. It is noteworthy that the considerations leading to the design of the deep architecture presented in this paper included this factor rather than having a sheer fixation on accuracy numbers. For most of the experiments, the models were designed to keep a computational budget of $1.5$ billion multiply-adds at inference time, so that the they do not end up to be a purely academic curiosity, but could be put to real world use, even on large datasets, at a reasonable cost.

In this paper, we will focus on an efficient deep neural network architecture for computer vision, codenamed “Inception”, which derives its name from the “Network in network” paper by Lin et al~\cite{lin2013nin} in conjunction with the famous ``we need to go deeper'' internet meme~\cite{knowyourmeme}. In our case, the word ``deep'' is used in two different meanings: first of all, in the sense that we introduce a new level of organization in the form of the ``Inception module'' and also in the more direct sense of increased network depth. In general, one can view the Inception model as a logical culmination of ~\cite{lin2013nin} while taking inspiration and guidance from the theoretical work by Arora et al~\cite{arora2013bounds}. The benefits of the architecture are experimentally verified on the ILSVRC 2014 classification and detection challenges, on which it significantly outperforms the current state of the art. 

\section{Related Work}

Starting with LeNet-5 ~\cite{lecun1989backprop}, convolutional neural networks (CNN) have typically had a standard structure -- stacked convolutional layers (optionally followed by contrast normalization and max-pooling) are followed by one or more fully-connected layers. Variants of this basic design are prevalent in the image classification literature and have yielded the best results to-date on MNIST, CIFAR and most notably on the ImageNet classification challenge~\cite{krizhevsky2012imagenet,zeiler2014visualizing}.  For larger datasets such as Imagenet, the recent trend has been to increase the number of layers ~\cite{lin2013nin} and layer size~\cite{zeiler2014visualizing,sermanet2013overfeat}, while using dropout~\cite{hinton2012dropout} to address the problem of overfitting.

Despite concerns that max-pooling layers result in loss of accurate spatial information, the same convolutional network architecture as ~\cite{krizhevsky2012imagenet} has also been successfully employed for localization~\cite{krizhevsky2012imagenet,sermanet2013overfeat}, object detection~\cite{girshick2014rich,sermanet2013overfeat,szegedy2013deep,erhan2014scalable} and human pose estimation~\cite{toshev2013deep}.
Inspired by a neuroscience model of the primate visual cortex, Serre et al.~\cite{serre2007robust} use a series of fixed Gabor filters of different sizes in order to handle multiple scales, similarly to the Inception model. However, contrary to the fixed 2-layer deep model of ~\cite{serre2007robust}, all filters in the Inception model are learned. Furthermore, Inception layers are repeated many times, leading to a 22-layer deep model in the case of the GoogLeNet model.

Network-in-Network is an approach proposed by Lin et al.~\cite{lin2013nin} in order to increase the representational power of neural networks. When applied to convolutional layers, the method could be viewed as additional $1\times 1$ convolutional layers followed typically by the rectified linear activation ~\cite{krizhevsky2012imagenet}. This enables it to be easily integrated in the current CNN pipelines. We use this approach heavily in our architecture. However, in our setting, $1\times 1$ convolutions have dual purpose: most critically, they are used mainly as dimension reduction modules to remove computational bottlenecks, that would otherwise limit the size of our networks.  This allows for not just increasing the depth, but also the width of our networks without significant performance penalty.

The current leading approach for object detection is the Regions with Convolutional Neural Networks (R-CNN) proposed by Girshick et al.~\cite{girshick2014rich}. R-CNN decomposes the overall detection problem into two subproblems: to first utilize low-level cues such as color and superpixel consistency for potential object proposals in a category-agnostic fashion, and to then use CNN classifiers to identify object categories at those locations. Such a two stage approach leverages the accuracy of bounding box segmentation with low-level cues, as well as the highly powerful classification power of state-of-the-art CNNs. We adopted a similar pipeline in our detection submissions, but have explored enhancements in both stages, such as multi-box ~\cite{erhan2014scalable} prediction for higher object bounding box recall, and ensemble approaches for better categorization of bounding box proposals.
