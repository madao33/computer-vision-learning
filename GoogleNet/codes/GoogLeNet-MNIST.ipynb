{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# GoogLeNet基于MNIST数据集的实现\r\n",
    "\r\n",
    "鉴于ImageNet数据集较为庞大，而本人的设备计算能力有限，主要是学习GoogLeNet的思想，这里就参考其他大佬的代码仿真一下GoogLeNet框架，直接通过tochvision获取MNIST数据集，相关的代码如下"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# 导入模块\r\n",
    "import torch.nn as nn\r\n",
    "import torch\r\n",
    "from torchvision import transforms, datasets\r\n",
    "import torchvision\r\n",
    "import json\r\n",
    "import torch.nn.functional as F\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import os\r\n",
    "import numpy as np\r\n",
    "import torch.optim as optim\r\n",
    "import time\r\n",
    "from torch.optim import lr_scheduler\r\n",
    "from sklearn.metrics import confusion_matrix\r\n",
    "import itertools\r\n",
    "import random"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "epochs = 50\r\n",
    "\r\n",
    "train_acc = []\r\n",
    "test_acc = []\r\n",
    "train_loss = []\r\n",
    "test_loss = []"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "classes = ('0','1','2','3','4','5','6','7','8','9')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "data_transform = {\r\n",
    "    \"train\": transforms.Compose([transforms.RandomResizedCrop(224),\r\n",
    "                                 transforms.RandomHorizontalFlip(),\r\n",
    "                                 transforms.ToTensor(),\r\n",
    "                                 transforms.Normalize((0.50,), (0.50,))]),\r\n",
    "    \"val\": transforms.Compose([transforms.Resize(256),\r\n",
    "                               transforms.CenterCrop(224),\r\n",
    "                               transforms.ToTensor(),\r\n",
    "                               transforms.Normalize((0.50,), (0.50,))])}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "train_dataset = torchvision.datasets.MNIST(root='../data',train=True,\r\n",
    "                                        download=True, transform=data_transform[\"train\"])\r\n",
    "validate_dataset = torchvision.datasets.MNIST(root='../data',train=False,\r\n",
    "                                       download=True,transform=data_transform[\"val\"])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 503: Service Unavailable\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ../data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Extracting ../data\\MNIST\\raw\\train-images-idx3-ubyte.gz to ../data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 503: Service Unavailable\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ../data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "102.8%\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Extracting ../data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ../data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 503: Service Unavailable\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ../data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Extracting ../data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ../data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "112.7%"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Extracting ../data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ../data\\MNIST\\raw\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n",
      "C:\\Users\\Administrator\\AppData\\Roaming\\Python\\Python36\\site-packages\\torchvision\\datasets\\mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "batch_size = 128\r\n",
    "train_num = len(train_dataset)\r\n",
    "val_num = len(validate_dataset)\r\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,\r\n",
    "                                           batch_size=batch_size, shuffle=True,\r\n",
    "                                           num_workers=8)\r\n",
    "validate_loader = torch.utils.data.DataLoader(validate_dataset,\r\n",
    "                                              batch_size=batch_size, shuffle=False,\r\n",
    "                                              num_workers=8)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\Administrator\\.conda\\envs\\python3.6env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4 (`cpuset` is not taken into account), which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# 绘制准确率\r\n",
    "def plot_acc_curves(array1, array2):\r\n",
    "    plt.figure(figsize=(10, 10))\r\n",
    "    x = np.linspace(1, epochs, epochs, endpoint=True)\r\n",
    "    plt.plot(x, array1, color='r', label='Train_accuracy')\r\n",
    "    plt.plot(x, array2, color='b', label='Test_accuracy')\r\n",
    "    plt.legend()\r\n",
    "    plt.title('accuracy of train and test sets in different epoch')\r\n",
    "\r\n",
    "    plt.xlabel('epoch')\r\n",
    "    plt.ylabel('accuracy: ')\r\n",
    "    plt.savefig(\"acc_curves\")\r\n",
    "    plt.show()\r\n",
    "    plt.clf()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# 绘制混淆矩阵\r\n",
    "def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\r\n",
    "    plt.figure(figsize=(10, 10))\r\n",
    "    if normalize:\r\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\r\n",
    "        print(\"Normalized confusion matrix\")\r\n",
    "    else:\r\n",
    "        print('Confusion matrix, without normalization')\r\n",
    "\r\n",
    "    print(cm)\r\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\r\n",
    "    plt.title(title)\r\n",
    "    plt.colorbar()\r\n",
    "    tick_marks = np.arange(len(classes))\r\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\r\n",
    "    plt.yticks(tick_marks, classes)\r\n",
    "    \r\n",
    "    fmt = '.2f' if normalize else 'd'\r\n",
    "    thresh = cm.max() / 2.\r\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\r\n",
    "        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\r\n",
    "\r\n",
    "    plt.tight_layout()\r\n",
    "    plt.ylabel('True label')\r\n",
    "    plt.xlabel('Predicted label')\r\n",
    "    plt.savefig(\"confusion_matrix\")\r\n",
    "    plt.clf()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "# 绘制结果\r\n",
    "@torch.no_grad()\r\n",
    "def get_all_preds(model, loader):\r\n",
    "\r\n",
    "    all_preds = torch.tensor([]).to(device)\r\n",
    "    model.to(device)\r\n",
    "    for batch in loader:\r\n",
    "        images, labels = batch\r\n",
    "        preds = model(images.to(device))\r\n",
    "        all_preds = torch.cat((all_preds, preds),dim=0)\r\n",
    "    return all_preds"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# 绘制查看多分类的图片\r\n",
    "def plot_misclf_imgs(candidates,gts_np,preds_np,classes):\r\n",
    "    size_figure_grid = 5  # a grid of 5 by 5\r\n",
    "    fig, ax = plt.subplots(size_figure_grid, size_figure_grid, figsize=(20, 20))\r\n",
    "\r\n",
    "    for i, j in itertools.product(range(size_figure_grid), range(size_figure_grid)):\r\n",
    "        ax[i, j].get_xaxis().set_visible(False)\r\n",
    "        ax[i, j].get_yaxis().set_visible(False)\r\n",
    "\r\n",
    "    for k in range(5 * 5): \r\n",
    "        i = k // 5\r\n",
    "        j = k % 5\r\n",
    "        idx = candidates[k]\r\n",
    "        img = validate_dataset[idx][0].numpy()\r\n",
    "        img = img[0]\r\n",
    "        ax[i, j].imshow((img), cmap='gray') \r\n",
    "        ax[i, j].set_title(\"Label:\"+str(classes[gts_np[idx]]), loc='left')\r\n",
    "        ax[i, j].set_title(\"Predict:\"+str(classes[preds_np[idx]]), loc='right')\r\n",
    "\r\n",
    "    plt.savefig(\"misclf_imgs\")\r\n",
    "    plt.clf()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "# 绘制损失函数\r\n",
    "def plot_loss_curves(array1, array2):\r\n",
    "    plt.figure(figsize=(10, 10))\r\n",
    "    x = np.linspace(1, epochs, epochs, endpoint=True)\r\n",
    "    plt.plot(x, array1, color='r', label='Train_loss')\r\n",
    "    plt.plot(x, array2, color='b', label='Test_loss')\r\n",
    "    plt.legend()\r\n",
    "    plt.title('loss of train and test sets in different epoch')\r\n",
    "\r\n",
    "    plt.xlabel('epoch')\r\n",
    "    plt.ylabel('loss: ')\r\n",
    "    plt.savefig(\"loss_curves\")\r\n",
    "    plt.show()\r\n",
    "    plt.clf()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "# 定义Inception模块\r\n",
    "class Inception(nn.Module):\r\n",
    "    def __init__(self, in_channels, ch1x1, ch3x3red, ch3x3, ch5x5red, ch5x5, pool_proj):\r\n",
    "        super(Inception, self).__init__()\r\n",
    "\r\n",
    "        self.branch1 = BasicConv2d(in_channels, ch1x1, kernel_size=1)\r\n",
    "\r\n",
    "        self.branch2 = nn.Sequential(\r\n",
    "            BasicConv2d(in_channels, ch3x3red, kernel_size=1),\r\n",
    "            BasicConv2d(ch3x3red, ch3x3, kernel_size=3, padding=1)\r\n",
    "        )\r\n",
    "\r\n",
    "        self.branch3 = nn.Sequential(\r\n",
    "            BasicConv2d(in_channels, ch5x5red, kernel_size=1),\r\n",
    "            BasicConv2d(ch5x5red, ch5x5, kernel_size=5, padding=2)\r\n",
    "        )\r\n",
    "\r\n",
    "        self.branch4 = nn.Sequential(\r\n",
    "            nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\r\n",
    "            BasicConv2d(in_channels, pool_proj, kernel_size=1)\r\n",
    "        )\r\n",
    "\r\n",
    "    # 前向传播\r\n",
    "    def forward(self, x):\r\n",
    "        branch1 = self.branch1(x)\r\n",
    "        branch2 = self.branch2(x)\r\n",
    "        branch3 = self.branch3(x)\r\n",
    "        branch4 = self.branch4(x)\r\n",
    "\r\n",
    "        outputs = [branch1, branch2, branch3, branch4]\r\n",
    "        return torch.cat(outputs, 1)\r\n",
    "\r\n",
    "\r\n",
    "class InceptionAux(nn.Module):\r\n",
    "    def __init__(self, in_channels, num_classes):\r\n",
    "        super(InceptionAux, self).__init__()\r\n",
    "        self.averagePool = nn.AvgPool2d(kernel_size=5, stride=3)\r\n",
    "        self.conv = BasicConv2d(in_channels, 128, kernel_size=1)\r\n",
    "\r\n",
    "        self.fc1 = nn.Linear(2048, 1024)\r\n",
    "        self.fc2 = nn.Linear(1024, num_classes)\r\n",
    "\r\n",
    " \r\n",
    "    def forward(self, x):\r\n",
    "        \r\n",
    "        x = self.averagePool(x)       \r\n",
    "        x = self.conv(x)       \r\n",
    "        x = torch.flatten(x, 1)\r\n",
    "        x = F.dropout(x, 0.5, training=self.training)        \r\n",
    "        x = F.relu(self.fc1(x), inplace=True)\r\n",
    "        x = F.dropout(x, 0.5, training=self.training)        \r\n",
    "        x = self.fc2(x)\r\n",
    "        \r\n",
    "        return x\r\n",
    "\r\n",
    "class BasicConv2d(nn.Module):\r\n",
    "    def __init__(self, in_channels, out_channels, **kwargs):\r\n",
    "        super(BasicConv2d, self).__init__()\r\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, **kwargs)\r\n",
    "        self.relu = nn.ReLU(inplace=True)\r\n",
    "\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        x = self.conv(x)\r\n",
    "        x = self.relu(x)\r\n",
    "        return x\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "class GoogLeNet(nn.Module):\r\n",
    "    def __init__(self, num_classes=10, aux_logits=True, init_weights=False):\r\n",
    "        super(GoogLeNet, self).__init__()\r\n",
    "        self.aux_logits = aux_logits\r\n",
    "\r\n",
    "        self.conv1 = BasicConv2d(1, 64, kernel_size=7, stride=2, padding=3)\r\n",
    "        self.maxpool1 = nn.MaxPool2d(3, stride=2, ceil_mode=True)\r\n",
    "\r\n",
    "        self.conv2 = BasicConv2d(64, 64, kernel_size=1)\r\n",
    "        self.conv3 = BasicConv2d(64, 192, kernel_size=3, padding=1)\r\n",
    "        self.maxpool2 = nn.MaxPool2d(3, stride=2, ceil_mode=True)\r\n",
    "\r\n",
    "        self.inception3a = Inception(192, 64, 96, 128, 16, 32, 32)\r\n",
    "        self.inception3b = Inception(256, 128, 128, 192, 32, 96, 64)\r\n",
    "        self.maxpool3 = nn.MaxPool2d(3, stride=2, ceil_mode=True)\r\n",
    "\r\n",
    "        self.inception4a = Inception(480, 192, 96, 208, 16, 48, 64)\r\n",
    "        self.inception4b = Inception(512, 160, 112, 224, 24, 64, 64)\r\n",
    "        self.inception4c = Inception(512, 128, 128, 256, 24, 64, 64)\r\n",
    "        self.inception4d = Inception(512, 112, 144, 288, 32, 64, 64)\r\n",
    "        self.inception4e = Inception(528, 256, 160, 320, 32, 128, 128)\r\n",
    "        self.maxpool4 = nn.MaxPool2d(3, stride=2, ceil_mode=True)\r\n",
    "\r\n",
    "        self.inception5a = Inception(832, 256, 160, 320, 32, 128, 128)\r\n",
    "        self.inception5b = Inception(832, 384, 192, 384, 48, 128, 128)\r\n",
    "\r\n",
    "        if self.aux_logits:\r\n",
    "            self.aux1 = InceptionAux(512, num_classes)\r\n",
    "            self.aux2 = InceptionAux(528, num_classes)\r\n",
    "\r\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\r\n",
    "        self.dropout = nn.Dropout(0.4)\r\n",
    "        self.fc = nn.Linear(1024, num_classes)\r\n",
    "        if init_weights:\r\n",
    "            self._initialize_weights()\r\n",
    "\r\n",
    "    # 前向传播\r\n",
    "    def forward(self, x):\r\n",
    "        \r\n",
    "        x = self.conv1(x)\r\n",
    "        x = self.maxpool1(x)       \r\n",
    "        x = self.conv2(x)       \r\n",
    "        x = self.conv3(x)       \r\n",
    "        x = self.maxpool2(x)\r\n",
    "\r\n",
    "        \r\n",
    "        x = self.inception3a(x)       \r\n",
    "        x = self.inception3b(x)\r\n",
    "        x = self.maxpool3(x)      \r\n",
    "        x = self.inception4a(x)\r\n",
    "        \r\n",
    "        if self.training and self.aux_logits:    \r\n",
    "            aux1 = self.aux1(x)\r\n",
    "\r\n",
    "        x = self.inception4b(x)       \r\n",
    "        x = self.inception4c(x)       \r\n",
    "        x = self.inception4d(x)\r\n",
    "        \r\n",
    "        if self.training and self.aux_logits:    \r\n",
    "            aux2 = self.aux2(x)\r\n",
    "\r\n",
    "        x = self.inception4e(x)       \r\n",
    "        x = self.maxpool4(x)       \r\n",
    "        x = self.inception5a(x)       \r\n",
    "        x = self.inception5b(x)\r\n",
    "      \r\n",
    "\r\n",
    "        x = self.avgpool(x)        \r\n",
    "        x = torch.flatten(x, 1)      \r\n",
    "        x = self.dropout(x)\r\n",
    "        x = self.fc(x)\r\n",
    "        \r\n",
    "        if self.training and self.aux_logits:   \r\n",
    "            return x, aux2, aux1\r\n",
    "        return x\r\n",
    "\r\n",
    "    # 权重初始化\r\n",
    "    def _initialize_weights(self):\r\n",
    "        for m in self.modules():\r\n",
    "            if isinstance(m, nn.Conv2d):\r\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\r\n",
    "                if m.bias is not None:\r\n",
    "                    nn.init.constant_(m.bias, 0)\r\n",
    "            elif isinstance(m, nn.Linear):\r\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\r\n",
    "                nn.init.constant_(m.bias, 0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "net = GoogLeNet(num_classes=10, aux_logits=True, init_weights=True)\r\n",
    "net.to(device)\r\n",
    "loss_function = nn.CrossEntropyLoss()\r\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.005)\r\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "best_acc = 0.0\r\n",
    "save_path = 'googleNet.pth'\r\n",
    "since = time.time()\r\n",
    "for epoch in range(epochs):\r\n",
    "    net.train()\r\n",
    "    running_loss = 0.0\r\n",
    "    running_corrects = 0\r\n",
    "    for step, data in enumerate(train_loader, start=0):\r\n",
    "        images, labels = data\r\n",
    "        optimizer.zero_grad()\r\n",
    "        logits, aux_logits2, aux_logits1 = net(images.to(device))\r\n",
    "        _, predict_y = torch.max(logits, dim=1)\r\n",
    "        loss0 = loss_function(logits, labels.to(device))\r\n",
    "        loss1 = loss_function(aux_logits1, labels.to(device))\r\n",
    "        loss2 = loss_function(aux_logits2, labels.to(device))\r\n",
    "        loss = loss0 + loss1 * 0.3 + loss2 * 0.3\r\n",
    "        loss.backward()\r\n",
    "        optimizer.step()\r\n",
    "\r\n",
    "        running_loss += loss.item()\r\n",
    "        running_corrects += (predict_y == labels.to(device)).sum().item()\r\n",
    "        rate = (step + 1) / len(train_loader)\r\n",
    "        a = \"*\" * int(rate * 50)\r\n",
    "        b = \".\" * int((1 - rate) * 50)\r\n",
    "        print(\"\\rtrain loss: {:^3.0f}%[{}->{}]{:.3f}\".format(int(rate * 100), a, b, loss), end=\"\")\r\n",
    "    print()\r\n",
    "    accurate_train = running_corrects / train_num\r\n",
    "    train_loss.append(running_loss / len(train_loader))\r\n",
    "    train_acc.append(accurate_train)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\Administrator\\.conda\\envs\\python3.6env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4 (`cpuset` is not taken into account), which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "C:\\Users\\Administrator\\.conda\\envs\\python3.6env\\lib\\site-packages\\torch\\nn\\functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ..\\c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train loss:  1 %[->.................................................]5.999"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "net.eval()\r\n",
    "    acc = 0.0  \r\n",
    "    acc_train = 0.0\r\n",
    "    Loss_val = 0.0\r\n",
    "    with torch.no_grad():\r\n",
    "        for test_step,data_test in enumerate(validate_loader, start=0):\r\n",
    "            test_images, test_labels = data_test\r\n",
    "            outputs = net(test_images.to(device))  \r\n",
    "            loss_val = loss_function(outputs,test_labels.to(device))\r\n",
    "            Loss_val +=loss_val.item()\r\n",
    "            predict_y = torch.max(outputs, dim=1)[1]\r\n",
    "            acc += (predict_y == test_labels.to(device)).sum().item()\r\n",
    "        accurate_test = acc / val_num\r\n",
    "        test_acc.append(accurate_test)\r\n",
    "        for step,data in enumerate(train_loader, start=0):\r\n",
    "           images, labels = data\r\n",
    "           outputs = net(images.to(device))  \r\n",
    "           predict_y = torch.max(outputs, dim=1)[1]\r\n",
    "           acc += (predict_y == labels.to(device)).sum().item()\r\n",
    "        accurate_train = acc_train / train_num\r\n",
    "        train_acc.append(accurate_train)\r\n",
    "        if accurate_test > best_acc:\r\n",
    "            best_acc=accurate_test\r\n",
    "            torch.save(net.state_dict(), save_path)\r\n",
    "        print('[epoch %d] train_loss: %.3f  test_accuracy: %.3f' %\r\n",
    "              (epoch + 1, running_loss / step, accurate_test))\r\n",
    "\r\n",
    "        test_loss.append(Loss_val/len(validate_loader))\r\n",
    "time_elapsed = time.time() - since"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "test_preds = get_all_preds(net, validate_loader).cpu()\r\n",
    "gts = validate_dataset.targets\r\n",
    "preds = test_preds.argmax(dim=1)\r\n",
    "gts_np = np.array(gts)\r\n",
    "preds_np = np.array(preds)\r\n",
    "mis_idxes = list(np.where(gts_np!= preds_np)[0])\r\n",
    "candidates = random.sample(mis_idxes,25)\r\n",
    "cm = confusion_matrix(validate_dataset.targets, test_preds.argmax(dim=1))\r\n",
    "\r\n",
    "plot_confusion_matrix(cm, classes)\r\n",
    "print('Training complete in {:.0f}m {:.0f}s'.format(\r\n",
    "        time_elapsed // 60, time_elapsed % 60))\r\n",
    "print('Best val Acc: {:4f}'.format(best_acc))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 绘制准确率\r\n",
    "plot_acc_curves(train_acc,test_acc)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 绘制损失函数\r\n",
    "plot_loss_curves(train_loss,test_loss)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 绘制部分样本\r\n",
    "plot_misclf_imgs(candidates,gts_np,preds_np,classes)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.6.2",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.2 64-bit ('python3.6env': conda)"
  },
  "interpreter": {
   "hash": "7835acacaeaf508b921ecd27357232a38d5f7dc52657613320ee88843bc3a5f4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}